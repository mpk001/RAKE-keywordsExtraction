<title>Quality control of wastewater for irrigated crop production. (Water reports - 10)</title>
<section>1</section>
Preface
It is well recognized that irrigated agriculture can play a key role in finding alternative uses for urban wastewater that is now contaminating many good quality river supplies. A key step is to understand how the wastewater can be safely used. A recent set of international guidelines has been developed by the World Health Organization (WHO) that describes the levels of treatment needed for the various agricultural-type uses. Reality is, however, that treatment facilities sufficient to meet these guidelines may be a decade or more away. As a result, the extent of water supply contamination is increasing and irrigated agriculture is now being assessed as to whether it is linked to the spread of diseases including cholera.
Interim steps need to be taken to control how irrigation water is used. Most of the focus of control has been centred on where direct wastewater irrigation occurs but there is also a need to restrict cropping in areas that have contaminated irrigation supplies. One of the limitations of such an effort is the lack of well defined health-related water quality standards for water actually used for irrigation.
This document reviews the availability of water quality standards and describes an interim approach to standards that emphasizes promotion of safe production areas for high-risk crops such as vegetables. The approach is to assess the quality of water actually being used for irrigation against a known standard. In this interim period, it is proposed to utilize the WHO Guidelines for sewage treatment plant design as irrigation water standards to make this assessment. It is recognized that these guidelines were not intended for such use and there are few, if any, data that link such standards to disease incidence. Considering that the present level of water contamination in many countries already seriously exceeds these guidelines, achieving them in vegetable production areas as an interim step would be a major step toward improving health conditions that result from irrigation practices.
Use of these guidelines is untested. Proposing them here is to begin a process of assessment and eventually working toward a slightly modified version of the guidelines. The approach described here is to utilize these interim guidelines to promote "safe production areas", not to operate a crop restriction programme.
The focus of this report is centred around procedures developed and studied in 1992 in a FAO project in Chile. Because of the concern for the spread of cholera and other diseases by irrigation practices in Chile, both the farmers and the government began an aggressive programme to assess the extent of water supply contamination and to assure the consumer that raw vegetables were being produced in a safe environment. The focus of these joint efforts was "prevention" and that focus is used throughout the document. In addition, the entire programme should be considered interim. When the proper treatment facilities for wastewater are completed such a programme can be abandoned.
It is hoped that this report will attract expert comment. Comments and suggestions for improvement of its practical application to the field would be welcomed and should be addressed to:
Chief, Water Resources, Development and Management Service
Land and Water Development Division
Food and Agriculture Organization of the United Nations
Viale delle Terme di Caracalla
00100 Rome, Italy
<section>2</section>
Acknowledgements
This document could not have been produced without the valuable assistance of the Chilean project team leader, Juan Carlos Cuchacovich, Director of the Department of Natural Resource Protection (DEPROREN), Servicio Agricola y Ganadero (SAG), Ministry of Agriculture, Santiago. The author wishes to express his deep gratitude and appreciation for his contribution to the paper.
Many of the basic data and concepts of the proposed approach in this paper have been developed by numerous professionals. Special appreciation goes to the staffs of WHO, World Bank and FAO who, through their efforts, have raised the awareness of the need to safely manage wastewater in order to gain its resource value. Of special note are Juan Sagardoy, Wulf Klohn and Fernando Chanduvi of the Water Service (AGLW) of FAO who had the vision to see that a start must be made now to control water contamination that is impacting the safety of irrigation water supplies. The author wishes to express his gratitude to Mr Fernando Chanduvi for his thorough review of the text and appropriate incorporation of comments and suggestions from all reviewers. Thanks are also due to Ms. Chrissi Redfern for her assistance in the preparation of the final text.
This document is intended to be one small step in that process. Hopefully the paper will inspire new approaches to an age old problem. The solutions may not, however, be based on the same approaches used before.
<section>3</section>
Chapter 1 - Introduction
The use of domestic wastewater for crop production has been practised for several centuries in one form or another. Prior to the 1940s, most wastewater use occurred on "sewage farms" or areas specifically designated for such use. One of the oldest in the world is the Werribee Farm which serves the City of Melbourne, Australia. This large well-managed farm was established in 1897 and is still in operation today, irrigating some 10 000 ha with wastewater. The impetus for these "sewage farms" was to minimize or prevent pollution in rivers and conserve water and nutrients to improve agriculture (Shuval, 1991). Few of these "sewage farms" still exist today; most were ill-conceived, inadequately funded and poorly regulated, and were eventually abandoned because of public health concerns.
In the mid 1940s, domestic wastewater use again gained increased attention, especially in arid and semi-arid areas that suffer from insufficient overall water supplies. Although the same early motivations for wastewater use remained, the newer areas using wastewater were focused on ensuring they minimized or prevented potential public health problems. The principal concern was use of wastewater on crops normally eaten raw. The change in focus was driven by a better understanding of public health problems and the desire to improve public health standards.
The need to improve public health protection prompted a number of state health departments in the United States to establish guidelines and regulations to control the public health aspects of wastewater use in agriculture. These initial guidelines provided a rational basis for continuing wastewater use by agriculture while meeting strict public health criteria. One important criterion was to restrict the use of partially treated sewage to crops that are generally cooked before being consumed and allow only water that has gone through advanced wastewater treatment and microbial disinfection to be applied to crops normally eaten raw.
Many nations adopted the very strict microbial standards for wastewater use that were developed in California (USA) and elsewhere. In reality these microbial standards were almost unattainable in most wastewater treatment systems, therefore many poorer or developing countries abandoned plans for wastewater use (Shuval et al., 1986a). The primary reason was the realization that producing effluent with a microbial quality sufficient for unrestricted irrigation required costly sophisticated treatment technology. Some of these countries shifted their focus in wastewater use to unrestricted areas of use coupled with crop restrictions. Most, however, did not have a strong institutional structure to control cropping. The result has been little improvement in public health conditions associated with wastewater use. Untreated or partially treated wastewater continues to be used directly for unrestricted irrigation or is discharged to surface water channels where unintended use by
agriculture occurs when water is appropriated for irrigation use.
Over the past 20 years there has been a strong revival of interest in the controlled use of wastewater for crop irrigation. In addition to consumer health protection, the main reasons are:
The last of the above reasons is driven by an increased public awareness of the need for clean water supplies and rivers. This perception, coupled with the population explosion in the urban areas, has resulted in strongly competing demands for water supplies, especially the best quality supplies. Agricultural and rural communities are often left to use the least desirable water supplies including those that have been contaminated with an increasing level of urban wastewater discharges. For example, Mexico is studying the cost-benefit of doubling irrigation with wastewater in the next decade. This would be a means of releasing clean water supplies to cover the domestic needs of nearly 30 million people (Cifuentes et al., 1991/92). This situation is likely to continue in many developing countries until reliable treatment and disposal works are in place. The level of contamination in rivers and irrigation water supplies may be a serious constraint for developing countries as
they strive to produce an adequate and safe food supply in the future.
During the next 20 years while reliable wastewater treatment facilities are planned and constructed, agricultural and water resource planners must face two dilemmas:
There is sufficient information and technology available to plan and execute a wastewater use scheme properly. The key to a successful programme is to control potential public health problems. The differences in approach are centred on where the control or application of public health standards takes place: the point of treatment and discharge; the area where wastewater is used; or the actual point of use for crop production. In reality, the lack of treatment and well defined areas using wastewater in most developing countries will focus control for the near term on the point of use.
The simplest approach is to control the quality of wastewater at its point of treatment and discharge. This places regulation and control at the institutional level as treatment is normally conducted by a public agency. The quality of the discharge can then be regulated to fit the type of use. This alternative assumes that the treatment system is well managed and maintained and produces a reliable quality of effluent. This approach is utilized in the United States, Canada, and Europe and in many cases requires an advanced level of treatment technology. In most developing countries for the present, the lack of treatment works makes this a long-term goal. New approaches to treatment technology in developing countries (Shuval et al., 1986a) will assist in implementing this technology sooner than originally planned, but financial constraints are still likely to make this a long-term effort.
The alternative to controlling the quality of wastewater at its point of treatment and discharge is to control the place where wastewater can be used. This alternative moves around the need for immediate treatment of the wastewater and places an emphasis on controlling where the discharge is used. Under this alternative, wastewater use would be within a defined area and the emphasis would shift to controlling the type of crop production in that area. This approach requires a broader based institutional structure and a strong ability to control cropping in the wastewater use area. The key element is a defined area where cropping restrictions can be practised. This approach is utilized in several developing countries including Tunisia, Mexico, Peru and Kuwait.
The two previous alternatives assume there is either a strict control of wastewater treatment or a well defined area of use. In most developing countries lack of treatment, poorly maintained treatment works, lack of well defined use areas and unrestricted discharges to rivers and canals make using these two alternatives ineffective in the near term. Until treatment works are installed and wastewater use areas defined, most developing countries will be faced with trying to control cropping on a broad scale. Where unrestricted discharges occur to rivers and canals and widespread water appropriation occurs from these water bodies, the dilemma will be whether or not a strong institutional structure is available to implement and enforce cropping restrictions on a broad scale. An alternative approach to crop restrictions is to identify safe production areas and then utilize market pressures to implement a programme that promotes a crop produced in this safe environment.
This document describes an approach that promotes safe production areas. The programme involves evaluating the quality of irrigation water presently used, identifying safe production areas for high-risk crops, such as vegetable crops, and, through a water quality certification programme, promoting the safety of that produce. The goal is to use market pressures to promote safe vegetable products. The programme described here is based upon on an effort in Chile (1992) to control the quality of water used in vegetable production as the irrigation water in Chile was identified as a major mechanism in the spread of cholera and other gastrointestinal diseases (FAO, 1993; Shuval, 1993).
<section>4</section>
Chapter 2 - Health risks associated with wastewater use
Types of pathogens present in wastewater
Pathogens that reach the field or crop
Pathogen survival under agricultural field conditions
Relative health risk from wastewater use
Agronomic conditions that minimize disease spread when wastewater is used for irrigation
Guidelines for public health protection during wastewater use
There are agronomic and economic benefits of wastewater use in agriculture. Irrigation with wastewater can increase the available water supply or release better quality supplies for alternative uses. In addition to these direct economic benefits that conserve natural resources, the fertilizer value of many wastewaters is important. FAO (1992) estimated that typical wastewater effluent from domestic sources could supply all of the nitrogen and much of the phosphorus and potassium that are normally required for agricultural crop production. In addition, micronutrients and organic matter also provide additional benefits.
There are many successful wastewater use schemes throughout the world where nutrient recycling is a major benefit to the project (Pescod and Arar, 1988; FAO, 1992). Rarely, however, is a scheme laid out or planned on the basis of nutrient recycling. The primary constraint to any wastewater use project is public health. Wastewater, especially domestic wastewater, contains pathogens which can cause disease spread when not managed properly. The primary objective of any wastewater use project must therefore be to minimize or eliminate potential health risks.
In most developing countries direct wastewater use projects are normally centred near large metropolitan areas. These schemes often only use a small percentage of the wastewater generated. The result is that indirect use of wastewater prevails inmost developing countries.
Indirect use occurs when treated, partially treated or untreated wastewater is discharged to reservoirs, rivers and canals that supply irrigation water to agriculture. Indirect use poses the same health risks as planned wastewater use projects, but may have a greater potential for health problems because the water user is unaware of the wastewater being present. Indirect use is likely to expand rapidly in the future as urban population growth outstrips the financial resources to build adequate treatment works. Where indirect use occurs, the primary objective must also be to ensure that it is in a manner than minimizes or eliminates potential health risks.
The health hazards associated with direct and indirect wastewater use are of two kinds: the rural health and safety problem for those working on the land or living on or near the land where the water is being used, and the risk that contaminated products from the wastewater use area may subsequently infect humans or animals through consumption or handling of the foodstuff or through secondary human contamination by consuming foodstuffs from animals that used the area (WHO, 1989).
The survival of pathogens and how they infect a new host needs to be understood in developing a programme to eliminate or minimize health risks. The importance and complexity of the rural health problem for those living and working where wastewater is used is beyond the scope of this document. The focus of this document will be on the concern with those who handle, prepare or eat the crop after it has been harvested. The health issues associated with wastewater use for the handlers, preparers and consumers of the crop can be broken down into a series of questions (each will be covered in more detail in subsequent sections of this document):
Types of pathogens present in wastewater
Wastewater or natural water supplies into which wastewater has been discharged, are likely to contain pathogenic organisms similar to those in the original human excreta. Disease prevention programmes have centred upon four groups of pathogens potentially present in such wastes: bacteria, viruses, protozoa and helminths. There have been extensive reviews published on the range of these pathogenic organisms normally found in human excreta and wastewater. The most complete reviews are Feachem et al. (1983), Rose (1986) and Shuval et al. (1986a). The following short discussion is extracted from those reviews and is presented to establish a basic understanding of the pathogens and their abundance.
Bacteria. The faeces of a healthy person contains large numbers of bacteria (> 10^10/g), most of which are not pathogenic. Pathogenic or potentially pathogenic bacteria are normally absent from a healthy intestine unless infection occurs. When infection occurs, large numbers of pathogenic bacteria will be passed in the faeces thus allowing the spread of infection to others. Diarrhoea is the most prevalent type of infection, with cholera the worst form. Typhoid, paratyphoid and other Salmonella type diseases are also caused by bacterial pathogens.
Viruses. Numerous viruses may infect humans and are passed in the faeces (> 10^9/g). Five groups of pathogenic excreted viruses are particularly important: adenoviruses, enteroviruses (including polioviruses), hepatitis A virus, reoviruses and diarrhoea-causing viruses (especially rotavirus).
Protozoa. Many species of protozoa can infect humans and cause diarrhoea and dysentery. Infective forms of these protozoa are often passed as cysts in the faeces and humans are infected when they ingest them. Only three species are considered to be pathogenic: Giardia lamblia, Balantidium coli and Entamoeba histolytica. An asymptomatic carrier state is common in all three and may be responsible for continued transmission.
Helminths. There are many species of parasitic worms or helminths that have human hosts. Some can cause serious illnesses and the ones that pass eggs or larval forms in the excreta are of importance in considering wastewater use. Most helminths do not multiply within the human host, a factor of great importance in understanding their transmission, the ways they cause disease and the effects that environmental change will have on their control. Often the developmental stages (life cycles) through which they pass before reinfecting humans are very complex. Those that have soil, water or plant life as one of their intermediate hosts are extremely important in any scheme where wastewater is used directly or indirectly.
The helminths are classified in two main groups: the roundworms (nematodes) and worms that are flat in cross section. The flatworm, in turn, may be divided into two groups: the tapeworms which form chains of helminths "segments" and the flukes which have a single, flat, unsegmented body. Most of the roundworms that infect humans and also the schistosome flukes have separate sexes. The result is that transmission depends upon infection with both male and female worms and upon meeting, mating and egg production within the human body.
Pathogens that reach the field or crop
All the pathogens discussed in the previous section have the potential to reach the field. From the time of excretion, the potential for all pathogens to cause infection usually declines due to their death or loss of infectivity. The ability of an excreted organism to survive outside the human body is referred to as its persistence. For all the organisms, survival is highly dependent on temperature with greatly increased persistence at lower temperatures.
The first exposure of excreted pathogenic organisms outside the body is usually in water. This blend with freshwater is often referred to as sewage. This sewage is then either subjected to treatment prior to discharge, used directly for crop production or discharged to a watercourse where indirect use then occurs downstream. There are many studies on the survival or persistence of excreted organisms in water and sewage. A summary is shown in Table 1.
Many bacterial populations decline exponentially so that 90 to 99 percent of the bacteria are lost relatively quickly. Survival of bacteria, like many other organisms, depends greatly on how hostile the environment is including other micro-organisms in the water that might provide competition or predation. Bacteria often survive longer in clean water than in dirty water but survival in excess of 50 days is most unlikely and at 20-30°C, 20-30 days is a more common maximum survival time.
Viral survival may be longer than bacterial survival and is greatly increased at lower temperatures. In the 20-30°C range, two months seems a typical survival time, whereas at around 10°C, nine months is a more realistic figure. There is evidence that virus survival is enhanced in polluted waters, presumably as a result of some protective effect that the viruses may receive when they are adsorbed onto suspended solid particles in dirty water.
TABLE 1: Survival times of excreted pathogens in freshwater and sewage at 20-30°C
Pathogen
Survival time (days)
Viruses^a
Enteroviruses^b
<120 but usually <50
Bacteria
Faecal coliform^a
<60 but usually <30
Salmonella spp.^a
<60 but usually <30
Shigella spp.^a
<30 but usually <10
Vibrio cholera^c
<30 but usually <10
Protozoa
Entamoeba histolytica cysts
<30 but usually <15
Helminths
Ascaris lumbriocoides eggs
Many months
a. In seawater, viral survival is less, and bacterial survival is very much less than in freshwater.
b. Includes polio-, echo-, and coxsackieviruses.
c. V. cholera survival in aqueous environments is still uncertain.
Source: Feachem et al. (1983).
Protozoal cysts are poor survivors in any environment. A likely maximum in sewage or polluted water would not exceed that shown in Table 1 for Entamoeba histolytica. Helminth eggs vary from the very fragile to the very persistent. One of the most persistent is the Ascaris egg which may survive for a year or more. The major concern for this helminth is that the soil is its intermediate host prior to reinfecting humans.
The survival times shown in Table 1 may be altered by the type or degree of wastewater treatment given the sewage water prior to use or discharge to a water body. Different treatment processes remove pathogens to varying degrees. What is not well understood in wastewater treatment systems is whether the treatment process produced an elevated level of hostile environment that accelerated the death of the organism or whether the treatment process had little effect on excreted pathogens and simply allowed the necessary time for natural die-off to occur independent of the treatment process.
The critical factor to consider for wastewater use is that most wastewater treatment plants were designed to reduce organic pollution of rivers and lakes and rarely are designed to remove all risks from pathogenic organisms. Therefore, regardless of the level of treatment provided, some pathogenic organisms will reach the agricultural fields when the water is used.
In instances where the sewage water has not received treatment, the level of pathogenic organisms is likely to be higher whether the use is occurring directly from raw sewage or from raw sewage that has been blended with other water supplies. In both instances, pathogenic organisms will reach the agricultural fields. These pathogenic organisms, as with treated sewage, have the potential to contaminate both the soil and the crop depending upon how the irrigation water is used. The critical element is to understand that whether treated, partially treated, or untreated water is used, pathogenic organisms are present and the use site must be managed in a manner that minimizes or eliminates the potential for disease transmission.
TABLE 2: Factors affecting survival time of enteric bacteria in soil
Soil factor
Effect on bacterial survival
Antagonism from soil microflora
Increased survival time in sterile soil
Moisture content
Greater survival time in moist soils and during times of high rainfall
Moisture-holding capacity
Survival time is less in sandy soils than in soils with greater water-holding capacity
Organic matter
Increased survival and possible regrowth when sufficient amounts of organic matter are present
pH
Shorter survival time in acid soils (pH 3-5) than in alkaline soils
Sunlight
Shorter survival time at soil surface
Temperature
Longer survival at low temperatures; longer survival in winter than in summer
Source: Shuval et al. (1986a) as adapted from Gerba et al. (1975).
Pathogen survival under agricultural field conditions
The literature on survival times of excreted pathogens in soil and on crop surfaces has been reviewed by Feachem et al. (1983) and Strauss (1985). As expected there was wide variability in reported survival times which reflects the influence of environmental and analytical factors. Table 2 describes several factors affecting survival time of bacteria in soil. Many of these factors may also affect survival of other pathogenic organisms.
Knowledge of the survival of pathogens in soil and on the crop allows an initial assessment of the risk of transmitting disease via produced foodstuff or through worker exposure. WHO (1989) presented a summary of the potential survival times in agricultural cropping environments (Table 3). WHO concludes that "Available evidence indicates that almost all excreted pathogens can survive in soil... for a sufficient length of time to pose potential risks to farm workers. Pathogens survive on crop surfaces for a shorter time than in the soil as they are less well protected from the harsh effects of sunlight and desiccation. Nevertheless, survival times can be long enough in some cases to pose potential risks to crop handlers and consumers, especially when survival times are longer than crop growing cycles as is often the case with vegetables". While the length of the crop growing cycle is important, equally important is the length of time since the last irrigation cycle (potential
exposure cycle). WHO (1989) points out that excreted pathogens, if they do enter an irrigated area with the irrigation water, have the potential to remain infectious for a considerable period of time thus steps must be taken to interrupt this infection cycle.
Relative health risk from wastewater use
The discussion in the previous sections show that a broad spectrum of pathogenic microorganisms including bacteria, viruses, helminths and protozoa is present in wastewater and they survive for days, weeks and at times months in the soil and on crops that come in contact with wastewater. Early approaches to measuring the health risk from these pathogenic micro-organisms centred on detection. Based upon the fact that these micro-organisms could survive, detection in any of these environments was sufficient to indicate that a public health problem existed. It was then assumed that such detection showed evidence that a real potential for disease transmission existed (Shuval et al., 1986a; Shuval, 1991). This is a "zero-risk" approach. Throughout the years a number of standards and guidelines have been developed on this zero-risk approach. This led to standards for wastewater use that approached those of drinking water especially where vegetable crops were being grown.
TABLE 3: Survival times of selected excreted pathogens in soil and on crop surfaces at 20-30°C
Pathogen
Survival time
In soil
On crops
Viruses
Enteroviruses^a
<100 but usually <20 days
<60 but usually <15 days
Bacteria
Faecal coliform
<70 but usually <20 days
<30 but usually <15 days
Salmonella spp.
<70 but usually <20 days
<30 but usually <15 days
Vibrio cholera
<20 but usually <10 days
<5 but usually <2 days
Protozoa
Entamoeba histolytica cysts
<20 but usually <10 days
<10 but usually < 2 days
Helminths
Ascaris lumbricoides eggs
Many months
<60 but usually <30 days
Hookworm larvae
<90 but usually <30 days
<30 but usually <10 days
Taenia saginata eggs
Many months
<60 but usually <30 days
Trichuris trichiura eggs
Many months
<60 but usually <30 days
^a Includes polio-, echo-, and coxsackieviruses.
Source: WHO (1 989) as summarized from Feachem et al. (1983).
TABLE 4: Effectiveness of enteric pathogens to cause infections through wastewater irrigation related to their epidemiological characteristics
Enteric pathogens
Persistence in environment
Minimum infective dose
Immunity
Concurrent routes of infection
Latency/soil development stage
Viruses
Medium
Low
Long
Mainly home contact and food or water
No
Bacteria
Short/Medium
Medium/High
Short/Medium
Mainly home contact and food or water
No
Protozoa
Short
Low/Medium
None/Little
Mainly home contact and food or water
No
Helminths
Long
Low
None/Little
Mainly soil contact outside home and food
Yes
Source: Shuval et al. (1986b).
Whether a person becomes infected actually depends on a number of additional factors, each of which adds to or diminishes the actual risk of infection. Feachem et al. (1983) and Shuval et al. (1986b) reviewed these factors and found several that are important for determining the relative health risk during wastewater use:
Shuval et al. (1986b) developed a theoretical epidemiological model based on the above factors. The model looked at their relationship to the probability that one of the four enteric pathogen groups described earlier would cause infections in humans through wastewater irrigation. The following factors were considered necessary to cause a high probability of infection:
Table 4 presents the summary of how Shuval et al. (1986b) rated the five factors when considering the enteric pathogen groups.
The Shuval model shows that helminth diseases, if they are endemic, will be very effectively transmitted by irrigation with raw wastewater. On the other hand, the enteric virus diseases should be the least effectively transmitted by irrigation with raw wastewater. The bacterial and protozoan diseases rank between these two extremes. Shuval et al. (1986b) ranked the pathogens in the following descending order of risk:
1. High: Helminths (the intestinal nematodes - Ascaris, Trichuris, hookworm and Taenia)
2. Lower: Bacterial infections (i.e. cholera, typhoid and shigellosis) and Protozoan infections (i.e. ameobiasis, giardiasis)
3. Least: Viral infections (viral gastroenteritis and infectious hepatitis)
This ranking is consistent with the theoretical considerations noted by Feachem et al. (1983) where the determinations were made on factors other than wastewater use. Shuval et al. (1986b) reviewed the available epidemiological evidence to determine whether the theoretical model fitted the empirical evidence. This review concluded that there is evidence of disease transmission in association with the use of raw or partially treated wastewater. This evidence points most strongly to the helminths as the number one problem, particularly in developing countries. There was limited transmission of bacterial and virus disease. The empirical evidence therefore points to the usefulness of the theoretical model and especially the priority ranking for the potential threat of disease transmission. The Shuval model (Table 4) and the rationale behind the ranking of pathogens shown above were reviewed in the World Bank/WHO-sponsored Engelberg Report (IRCWD, 1985) that obtained the
endorsement of an international group of environmental experts and epidemiologists.
Agronomic conditions that minimize disease spread when wastewater is used for irrigation
The previous discussions demonstrate that a potential for disease transmission exists when wastewater is used for irrigation. Pathogens that are brought in with the wastewater can survive in the soil or on the crop. The actual risk of disease transmission, however, is related to whether this survival time is long enough to allow transmission to a susceptible host. The crop and the field are the link between the pathogen in the wastewater and the potential for infection. The factors controlling transmission of disease are agronomic, such as the crop grown, the irrigation method used to apply the wastewater, and the cultural and harvesting practices used.
The choice of crops for wastewater use areas depends upon a number of factors. The crop grown must be suitable to the agronomic conditions in the area. Determining factors include climate, soils, available water, pest control, marketing and farmer skills. These and other general agronomic problems are discussed in numerous publications and will not be discussed in detail here. Another factor of importance for wastewater use areas is water quality. The impact can be on the soil, on crop growth, or it can affect the consumer of that crop. Water quality impacts on the soil and on crop growth are discussed in detail in Ayers and Westcot (FAO, 1985) and will not be covered here. The microbiological quality of the water can directly affect the consumer of that crop because of the risk of infection from that crop. Shuval et al. (1986a) defined three levels of risk in selecting a crop to be grown. They are presented here in increasing order of public health risk:
Low(est) risk to consumer but field worker protection still needed
1. Crops not for human consumption (for example cotton, sisal).
2. Crops normally processed by heat or drying before human consumption (grains, oilseeds, sugar beet).
3. Vegetables and fruit grown exclusively for canning or other processing that effectively destroys pathogens.
4. Fodder crops and other animal feed crops that are sun-dried and harvested before consumption by animals.
5. Landscape irrigation in fenced areas without public access (nurseries, forests, green belts).
Increased risk to consumer and handler
1. Pasture, green fodder crops.
2. Crops for human consumption that do not come into direct contact with wastewater, on condition that none must be picked off the ground and that spray irrigation must not be used (tree crops, vineyards, etc.).
3. Crops for human consumption normally eaten only after cooking (potatoes, eggplant, beetroot).
4. Crops for human consumption, the peel of which is not eaten (melons, citrus fruits, bananas, nuts, groundnuts).
5. Any crop not identified as high-risk if sprinkler irrigation is used.
Highest risk to consumer, field worker and handler
1. Any crops eaten uncooked and grown in close contact with wastewater effluent (fresh vegetables such as lettuce or carrots, or spray-irrigated fruit).
2. Landscape irrigation with public access (parks, lawns, golf courses).
Another path of infection is from direct contact with the crop or soil in the area where wastewater was used. This path is directly related to the level of protection needed for field workers. The only feasible means of dealing with the worker safety problem is prevention. The following are a few of many low and high risk situations:
Low risk of infection
Mechanized cultural practices
Mechanized harvesting practices
Crop is dried prior to harvesting
Long dry periods between irrigations
High risk of infection
High dust areas
Hand cultivation
Hand harvest of food crops
Moving sprinkler equipment
Direct contact with irrigation water
Guidelines for public health protection during wastewater use
International guidelines or standards for the microbiological quality of irrigation water used on a particular crop do not exist. The reason is the lack of direct epidemiological data to show any relationship between the quality of water actually applied at the field level and disease transmission or infection. The only known guideline is from the US Environmental Protection Agency (prepared by the US National Academy of Sciences) which establishes the maximum acceptable level for irrigation with natural surface water, including river water, at 1000 faecal coliforms per 100 ml. This was based on testing of a limited number of rivers and canals used for irrigation between 1965 and 1972 and focused on the presence of pathogens not on epidemiological data (US EPA, 1973).
The lack of direct epidemiological data resulted in standards and guidelines for the quality of wastewater used for irrigation to be focused on effluent standards at the wastewater treatment plant rather than the quality at the point of use. These effluent standards have generally specified both maximum concentrations of bacteria and minimum treatment levels according to the class of crop to be irrigated (consumed vs non-consumable). These standards are most often used for process control at wastewater treatment plants. There have been few checks made of the actual microbiological quality of the water at the place where it is used for irrigation.
The earliest effluent standards for wastewater treatment plants were expressed in terms of the maximum permissible number of faecal coliform bacteria. In practice, faecal coliform count was a reasonable indicator of bacterial pathogens. Their environmental survival characteristics and rates of removal or die-off in treatment processes were similar. Faecal coliforms therefore made good indicators of treatment efficiencies (WHO, 1989). As technology advanced in wastewater treatment and disinfection, stricter effluent standards were often adopted without regard to the risks associated with use of the water. Standards as recent as 20 years ago were based on a "zero-risk" concept with the aim to achieve in the effluent a pathogen or microbial-free environment without regard to pathogen-host relationships or to valid epidemiological evidence of disease transmission caused by the practice of wastewater use (Hespanhol and Prost, 1994). Theoretically, these technology-based standards
could be met, therefore the maximum permissible levels were set correspondingly low in countries with this advanced level of technology. For example, the California (USA) State Health Department adopted a bacterial standard for unrestricted wastewater irrigation of <2.2 total coliforms/100 ml which was close to the existing drinking water standard. Many countries followed this lead and adopted the same criteria with little or no adaptation to local constraints or to the level of technology available to meet this standard (Shuval, 1991).
WHO, supported by a group of specialists, recognized that the extremely strict California standard for wastewater use that was being adopted by many countries was not justified by the available epidemiological evidence nor was it likely that many countries, especially developing countries, could meet this strict standard. The WHO group of experts recommended a microbial guideline for unrestricted irrigation of all crops of not more than 100 total coliforms per 100 ml (WHO, 1973). This was a significant liberalization. The WHO group of experts also recognized the lack of sound epidemiological data and recommended that future wastewater irrigation guidelines be given a sounder epidemiological basis.
Extensive epidemiological evidence has been accumulated since the initial 1973 WHO Guidelines (Feachem et al., 1983; Blum and Feachem, 1985; Rose, 1986; Shuval et al., 1986a). This evidence was reviewed at international meetings in Engelberg (IRCWD, 1985) and Adelboden (Mara and Cairncross, 1989). The consensus of health experts is that the actual risk associated with irrigation with treated wastewater is much lower than previously estimated particularly with respect to bacterial pathogens. On the other hand, they raised the level of concern for parasitic diseases which they felt were the main risk for individual and overall public health associated with the use of insufficiently treated wastewater in agriculture. This is consistent with the relative risk assessment presented in Table 4.
Based on an epidemiological review, a WHO Scientific Group on Health Guidelines for the Use of Wastewater in Agriculture and Aquaculture adopted the microbiological quality guidelines for wastewater use in agriculture shown in Table 5 (WHO, 1989). These new guidelines recommend less stringent values for faecal coliforms than were previously recommended by WHO in 1973. The new guidelines are stricter than previous standards concerning the need to reduce helminth egg concentrations in effluent. The guidelines do not refer specifically to protozoa. It was implied that if the helminth egg level could be reached that equally high removals of all protozoa will be achieved. The purpose of applying the helminth standard throughout all cropping systems was to increase the level of protection for agricultural workers, who are at high risk from intestinal nematode infection (Mara and Cairncross, 1989). The review also concluded that no bacterial guideline was needed for protection of
the agricultural worker since there was little evidence indicating a risk to such workers from bacteria; the WHO Scientific Group expected some degree of reduction in bacterial concentration associated with efforts to meet the helminth reduction levels (Figure 1) (WHO, 1989).
It is important to remember that the guidelines in Table 5 are for the microbiological quality of treated effluent from a wastewater treatment plant when that water is intended for crop irrigation. The WHO Scientific Group on Health Guidelines intended the guidelines in Table 5 as design goals in planning wastewater treatment plants and they were not intended as standards for quality surveillance or routine monitoring of irrigation water (Mara and Cairncross, 1989). Reality is, however, that planning, design and construction of wastewater treatment facilities that can consistently meet the present WHO Guidelines will be decades in the making.
With exploding urban populations, the degree of river and irrigation water supply contamination in developing countries will likely increase. Pressure will also increase to utilize partially treated wastewater for irrigation until adequate treatment facilities can be constructed. Because of this increasing level of irrigation water contamination there is an immediate need to control wastewater use in high risk cropping systems such as vegetable crop production. Adequate control, however, can only come about when guidelines or regulations are in place that define the quality of water that can be safely applied to the cropland. The present guidelines of WHO, although intended as design goals for wastewater treatment plants, could be used as interim irrigation water standards for regulating cropping practices. These guidelines could be applied in areas where wastewater is utilized directly for irrigation or where use is indirect by diversion of contaminated river water
supplies.
Even though there is a lack of data to define whether the WHO Guidelines could be used as irrigation water standards, their potential use is implied in the WHO discussion of handling partially treated wastewater, which stated, "A lesser degree of removal [than needed to achieve the recommended guideline quality for unrestricted irrigation] can be accepted if other health protection measures are envisaged, or if the quality of the wastewater will be further improved after treatment, whether by dilution in naturally occurring waters, by prolonged storage or by transport over long distances in a river or canal" (WHO, 1989). Bartone (1991) in a review of effluent irrigation also implied that if the WHO Guidelines were routinely applied, no undue health risk of infectious disease transmission in effluent irrigation projects should arise. These statements recognize the importance of at least partial treatment and other steps that may occur prior to irrigation use.
TABLE 5: Recommended microbiological quality guidelines for wastewater use in agriculture^a
Category
Reuse condition
Exposed group
Intestinal nematodes^b (arithmetic mean no. of eggs per litre^c)
Faecal coliforms (geometric mean no. per 100 ml^c)
Wastewater treatment expected to achieve the required micro-biological quality
A
Irrigation of crops likely to be eaten uncooked, sports fields, public parks^d
Workers, consumers, public
£ 1000^d
A series of stabilization ponds designed to achieve the microbiological quality indicated, or equivalent treatment
B
Irrigation of cereal crops, industrial crops, fodder crops, pasture and trees^e
Workers
No standard recommended
Retention in stabilization ponds for 8-10 days or equivalent helminth and faecal coliform removal
C
Localized irrigation of crops in cat. B if exposure of workers and the public does not occur
None
Not applicable
Not applicable
Pretreatment as required by the irrigation technology, but not less than primary sedimentation
^a In specific cases, local epidemiological, socio-cultural and environmental factors should be taken into account, and the guidelines modified accordingly.
^b Ascaris and Trichuris species and hookworms.
^c During the irrigation period.
^d A more stringent guideline (£ 200 faecal coliforms per 100 ml) is appropriate for public lawns, such as hotel lawns, with which the public may come into direct contact.
^e In the case of fruit trees, irrigation should cease two weeks before fruit is picked, and no fruit should be picked off the ground. Sprinkler irrigation should not be used.
Source: WHO (1989).
FIGURE 1: Generalized removal curves for BOD, helminth eggs, excreted bacteria, and viruses in waste stabilization ponds at temperatures above 20°C
(Source: Shuval et al., 1986b)
In spite of the lack of experience in using the present WHO Guidelines as irrigation water standards or for routine monitoring of areas directly or indirectly using wastewater for irrigation, the present situation requires that interim irrigation water standards be established. Until sufficient epidemiological information is available, it seems prudent to utilize the 1989 WHO Guidelines for controlling the quality of water used to irrigate vegetable or other high-risk crops. These Guidelines should not be considered a level to which quality can deteriorate, rather they should be a performance goal to achieve for those water supplies which presently exceed this level. The goal would be to control the use of wastewater in cropping areas that would present a high risk of disease spread. Using the WHO Guidelines as irrigation standards would help to:
Shuval et al. (1986b) stressed that a major or total reduction in negative health effects could be made if the greatest emphasis is placed on helminth egg removal during wastewater treatment. The dilemma is that little or no experience is available in using helminth egg concentration in irrigation water monitoring nor are there well understood monitoring techniques available. Because of this shortcoming, the initial emphasis in using the WHO Guidelines should focus on the faecal coliform guideline. Monitoring and evaluation techniques for faecal coliforms are well understood.
The faecal coliform level defined in the WHO Guidelines is already being used in the USA as a water quality guideline. The US Environmental Protection Agency (EPA), together with the National Academy of Sciences (NAS), have recommended that the acceptable guideline for irrigation with natural surface water, including river water containing wastewater discharges, be set at 1000 faecal coliforms per 100 ml (US EPA, 1973). The US EPA level is also consistent with the 1000-2000 faecal coliforms per 100 ml level used as a standard for bathing in Europe (WHO, 1989). The US EPA Guideline has been adopted in some countries as an irrigation water quality standard. For example in Chile, NCh 1333 dated 1978 establishes the US EPA faecal coliform level as an irrigation standard.
It must not be implied that the recommendation to use only the faecal coliform guideline as the irrigation water standard would be equivalent to the WHO Guidelines. WHO has stressed the use of the helminth egg level also, but a lack of experience in applying this helminth guideline makes it difficult to implement for routine surveillance. It is unclear whether using faecal coliform as the only irrigation standard would pose the same or higher risk than a similar concentration coming from a wastewater treatment plant. WHO (1989) feels the wastewater treatment process lowers the helminth egg level but it is unclear whether the same action would occur with untreated or partially treated wastewater that is diluted in natural river flow or where bacterial die-off has occurred. This concern demonstrates a potential shortcoming or criticism of using only the faecal coliform portion of the WHO Guidelines as an irrigation water standard.
TABLE 6: Faecal coliforms in rivers
Number of faecal coliforms per 100 ml
No. of rivers tested in each region
North America
Central and South America
Europe
Asia and the Pacific
Total number of rivers
Source: WHO (1989).
There are no helminth egg data available on most rivers that are carrying a percentage of partially or untreated wastewater. Considerable data are available for faecal coliform levels. Table 6 shows that in about 45 percent of the 110 rivers tested throughout the world, the faecal coliform levels exceeded the WHO Guideline, illustrating that river contamination levels are already high and not likely to improve rapidly until treatment facilities are built.
Programmes to reduce risk often focus on the most highly contaminated waters first. Table 6 shows that nearly 15 percent of the rivers tested worldwide had faecal coliform levels ten or more times greater than the WHO Guidelines. Water from such rivers is widely used for irrigation without any restrictions on its use.
Dramatic initial success in disease reduction can be achieved by concentrating efforts in the worst contaminated areas. As with Chile, however, disease rates still remain high and expanding crop restrictions to a more widespread area will be difficult. As contamination levels are expected to remain high for the foreseeable future, there needs to be an equal emphasis on defining and promoting safe production areas for the high-risk crops such as vegetable crops. A discussion of how to utilize irrigation water quality guidelines to define these safe production areas is given in Chapter 4.
IMPACT OF WASTEWATER DISCHARGES ON RIVER WATER QUALITY
The impact of treatment works can be seen in contrasting examples. In the irrigated areas that surround Metropolitan Santiago, Chile, greater than 60% of the irrigated area is diverting river water with faecal coliform levels in excess of the WHO Guidelines. The cause of these high levels is untreated and unrestricted discharges into the rivers (Figure 2). Chile has begun (1992) a vigorous programme to implement adequate treatment facilities, but this programme is likely to take 10-20 years or more to complete. In contrast, tests of the quality of river water used in North America for unrestricted irrigation show > 90% of the rivers had faecal coliform levels below the WHO Guidelines (Table 6). These levels, however, are only being achieved after an aggressive and costly programme over the last 30 years to upgrade wastewater treatment facilities.
FIGURE 2: Percentage of the irrigated area affected by various levels of faecal contamination in the source of irrigation supply water within the Metropolitan Region of Chile (Source: FAO, 1993)
IMPACT OF CROP RESTRICTIONS ON DISEASE LEVELS NEAR SANTIAGO, CHILE
For example, Figure 2 shows that almost 60% of the irrigated area within the Metropolitan Region of Chile (Santiago) uses water in excess of 10 000 faecal coliforms per 100 ml. In 1992, as a result of a cholera outbreak, the Government of Chile began a vigorous crop restriction programme in the areas with the worst contamination. Preliminary data show that, in addition to controlling the cholera outbreak, there has been a dramatic decline in the cases of hepatitis and typhus (Figure 3).
FIGURE 3: Cases of hepatitis and typhus reported in Chile (Source: FAO, 1993)
<section>5</section>
Chapter 3 - Implementing health protection measures for wastewater use
Wastewater treatment to lower health risks
Lowering risk of direct human exposure in areas using wastewater
Lowering risk to consumers through crop restrictions
The discussion to this point has centred on defining the risk of disease transmission. All wastewater contains pathogens and these pathogens do pose a risk. As discussed in Chapter 2, that risk can be defined. The focus now shifts to evaluating what can be done to minimize or eliminate that risk.
The water is the means that allows an infectious pathogen to move to a new host. The intermediate step in this process is crop production which can provide a route of infection.
There are two approaches to developing a regulatory programme for health protection. The first is to focus on lowering the risk from the water. This is normally done by wastewater treatment or treatment and disinfection. Where the treated water does not meet health protection standards for unrestricted irrigation, the focal point for risk reduction shifts to the point of water use (irrigation). Here agricultural restrictions can lower the potential health risks. The point of water use is usually where the route of infection shifts to the soil and crop; therefore, these become the primary focus of management or regulatory strategies.
There are numerous agronomic practices that can assist in lowering the risk from wastewater use but most of these are individual site decisions that are normally made by the farmer to increase agricultural production and not to lower the overall disease infection risk. Farmers cannot be expected to implement a programme that focuses on individual cultural practices since the farming goal is agricultural production. Any regulatory approach must be institutional and have a primary focus on the type of crop grown. Such an approach avoids the regulation process being involved with the way a particular crop is grown.
The following sections briefly describe the two levels of approach: wastewater treatment and control at the field level. The latter is divided into the steps needed to prevent worker safety problems and those needed to prevent infection of the consumer of the crop.
Wastewater treatment to lower health risks
The water is the vehicle for movement of any pathogenic organism in wastewater. Any regulatory programme must first focus on intercepting these pathogens and rendering them harmless. The first option is to provide treatment of the wastewater. There is no perfect treatment process but the long-term goal should be to reduce the risk from the wastewater by meeting the guidelines adopted by WHO (1989). If the treatment process is capable of consistently meeting the WHO Guidelines then the effluent wastewater should be safe for unrestricted irrigation. It should be remembered that the wastewater is still a vehicle for transmission of pathogens as it is not a pathogen-free environment but it should pose an insignificant risk of disease infection when used properly for crop irrigation (WHO, 1989).
TABLE 7: Qualitative comparison of various wastewater treatment systems
Criteria
Package plant
Activated sludge plant
Extended aeration activated sludge
Biological filter
Oxidation ditch
Aerated lagoon
Waste stabilization pond system
Plant performance
BOD removal
F
F
F
F
G
G
G
FC removal
P
P
F
P
F
G
G
SS removal
F
G
G
G
G
F
F
Helminth removal
P
F
P
P
F
F
G
Virus removal
P
F
P
P
F
G
G
Economic factors
Simple and cheap construction
P
P
P
P
F
F
G
Simple operation
P
P
P
F
F
P
G
Land requirement
G
G
G
G
G
F
P
Maintenance costs
P
P
P
F
P
P
G
Energy demand
P
P
P
F
P
P
G
Sludge removal costs
P
F
F
F
P
F
G
Key: FC = Faecal coliform; SS = Suspended solids: G = Good: F = Fair: P = Poor
Source: Arthur (1983).
TABLE 8: Expected removal of enteric pathogenic micro-organisms in various wastewater system
Treatment process
Removal (log[10] units) of (i.e., 4 log[10] units, equivalent to = 10^-4 = 99.9 percent removal)
Bacteria
Helminths
Viruses
Cysts
Primary sedimentation
Plain
Chemically Assisted^a
1-3^g
Activated sludge^b
Biofiltration^c
Aerated lagoon^c
1-3^g
Oxidation ditch^b
Disinfection^d
2-6^g
Waste stabilization ponds^e
1-6^g
1-3^g
Effluent storage reservoirs^f
1-6^h
1-3^h
^a Further research is needed to confirm performance.
^b Including secondary sedimentation.
^c Including settling pond.
^d Chlorination or ozonation.
^e Performance depends on number of ponds in series and other environmental factors.
^f Performance depends on retention time, which varies with demand.
^g With good design and proper operation the recommended guidelines are achievable.
Source: Mara and Cairncross (1989).
The most appropriate wastewater treatment is that which will produce an effluent meeting the recommended microbial guidelines both at a low cost and with minimal operational and maintenance requirements (Pescod and Arar, 1988). Table 7 gives a general qualitative comparison of various types of wastewater treatment systems in use today. Good reviews of wastewater treatment processes are found in FAO (1992), Shuval et al. (1986a) and WHO (1989).
The degree of removal of micro-organisms from wastewater by a treatment process is best expressed in terms of log[10] units (e.g., a reduction of 4 log[10] units = 10^-4 = 99.9% removal). To achieve the recommended WHO Guidelines for unrestricted irrigation, a reduction in the bacterial concentration of at least 4 log[10] units is required along with the need to achieve a reduction in the helminth egg concentration of 3 log[10] units (WHO, 1989). Table 8 gives the expected removal of various pathogens from typical wastewater treatment systems using the log[10] units.
Lowering risk of direct human exposure in areas using wastewater
In the area where wastewater is used directly or indirectly for crop production, three groups are at risk of disease infection:
· agricultural workers and their families;
· crop handlers; and
· those living near the areas irrigated with wastewater.
These rural groups carry the same relative risk of exposure as was shown in the epidemiological model of Shuval et al. (1986b). The greatest risk is from helminth infections and, as a result of repeated exposure, these rural groups likely build up high infection doses that are then transmitted to others in their community (Mara and Cairncross, 1989).
Lowering the potential for disease transmission must first focus on the source of contamination - the water. Wastewater treatment for helminth control must be a priority. In addition to treatment, all wastewater use schemes should have a control programme aimed at the rural population, especially the agricultural worker. The goal is to either prevent direct contact with the pathogens, or prevent any contact leading to disease. Measures to protect the agricultural field worker and the crop handlers include wearing protective clothing especially gloves and shoes (to prevent contact with pathogens), maintaining high levels of hygiene (to remove any pathogens present) and possibly immunizations (to prevent infection leading to disease). Local residents should be kept fully informed about where wastewater is used so they may avoid the area. Of equal importance is to educate workers, residents and others not to use canals or other wastewater facilities for drinking or domestic
purposes. A broader discussion of these preventative measures is presented in WHO (1989).
FIGURE 4: Generalized model to show the level of risk to human health associated with different combinations of control measures for the use of wastewater in agriculture
(Source: Blumenthal et al. (1989) and Mara and Cairncross (1989))
Lowering risk to consumers through crop restrictions
The first approach to lowering the health risk from wastewater use in agriculture is by adequate wastewater treatment. In reality, in most developing countries, wastewater treatment to the levels proposed in the WHO Guidelines (Table 5) is a long-term goal. In the interim, until treatment facilities are operating, widespread unrestricted use with untreated or partially treated wastewater will continue. Temporary steps need to be taken to improve the existing situation. Along with building treatment facilities, an equal importance needs to be given to the second level of approach which is managing the wastewater use area to ensure this is not the source of infectious diseases.
Once the wastewater is applied, the field and the crop become the vehicle of infectious exposure. The field is the route of exposure to the agricultural worker (see previous section) and the crop becomes the route of exposure to the consumer of that crop. The generalized model used by WHO (Figure 4) demonstrates the relative risk to human health when using wastewater and shows that cropping restrictions can be an effective measure to protect the consumer. Strauss (1991) reviewed the application of the control measures model to several wastewater use areas worldwide and concluded that the model was an effective planning tool.
Crop restriction is the most widely used measure to protect public health because it provides protection for both the general population and population groups that may have a lower resistance to infection. This latter group includes those not part of the indigenous population such as tourists or persons outside the country when produce is being exported to other countries or regions. The focus of crop restriction has been on salad or vegetable crops that are normally eaten raw. Recently, however, other crops have come under concern because of the introduction of pathogens into the home from wastewater irrigated fields. Many of the root crops and crops grown in contact with the ground (melons) are suspect (Shuval et al., 1986a).
Many feel that crop restrictions are administratively unattainable (Shuval et al., 1986a) and need a strong institutional framework along with a capacity to monitor and control compliance with the regulations (WHO, 1989). Mara and Cairncross (1989) discuss the strengths and weaknesses of a crop restriction programme and point to five factors that will contribute to a successful programme:
The success of a crop restriction programme depends greatly on how many users there are and whether the wastewater use occurs within a defined area. Crop restriction is relatively simple to implement where the wastewater is used by a small number of large farms, whether they are private farms, cooperatives, state farms, or operated by the wastewater authority. Such an arrangement allows regulation to occur within a specified area. Knowing where the water is being used is a key factor in an effective programme. Enforcement of crop restrictions on a large number of small farms will be more difficult but, if they are within a defined area, control is much easier.
Crop restriction is easiest to implement when the wastewater use scheme, or at least the distribution of the wastewater, is centrally managed by an irrigation association or the wastewater authority. Centralized control of the wastewater distribution makes control much simpler regardless of the number of farmers utilizing the water and it implies that the wastewater is being used in a defined area. Centralized control removes many of the unknowns that make field level enforcement difficult.
Reality in many developing countries, however, is that the wastewater - treated, partially treated and untreated - is discharged directly to surface waters and these are again diverted downstream for irrigation purposes. This unrestricted discharge leads to widespread distribution of the wastewater and makes crop restriction extremely difficult. In this situation, it is essential to enforce strict control over the effluent quality being discharged but, as discussed previously, most treatment systems in developing countries are not capable of producing a consistent effluent quality or there are no treatment works in place. This means raw sewage or partially treated wastewater is being discharged, diluted and distributed throughout the irrigation network.
In addition to large unrestricted discharges occurring from the urban centres, secondary discharges are also having an impact on irrigation water quality. Secondary discharges are those that occur into irrigation canals after the irrigation water is diverted from the main surface water supply.
Lack of adequate treatment works, continued unrestricted distribution of wastewater and the impact of secondary discharges in developing countries make a reduction in infection potential at wastewater use sites difficult without some control on cropping. Without controls on cropping, the Agricultural Ministry in developing countries, in the near-term, may not be able to provide sufficient, safe, vegetable products to meet national needs or for export. In addition, as the world population becomes increasingly aware of the need for clean water and clean food products, the ability of the Agricultural Ministry to meet this demand will be greatly diminished.
In the interim, until adequate and reliable wastewater treatment facilities are completed or well defined use areas established, a national programme needs to be established to identify large irrigated areas that can be safely used to meet national and export vegetable production goals without having to implement a large surveillance and enforcement programme that is usually associated with crop restrictions. This programme should be a joint programme between the Health and Agricultural Ministries with the goal to:
Chapter 4 discusses an approach to promoting such safe production areas.
<section>6</section>
Chapter 4 - Developing a programme to promote safe production
Phase I: Development of a sound information base
Phase II: Organization and analysis of the information
Phase III: Certification programmes and institutional and policy issues
As discussed in Chapter 3, adopting a crop restriction programme as a means of health protection where wastewater is used for irrigation requires a strong institutional framework. It also requires the capacity to monitor and control compliance with regulations and to enforce them. In reality, if the wastewater is not used in a defined and restricted area and the use of that water is not centrally controlled, enforcement of a crop restriction programme at the field level will not be easy to accomplish.
The key to any effective programme is the farmer. Maintaining cooperation of the farmers involves ensuring that there is a strong market for the crops allowed and ensuring that there are negative market pressures for the restricted or high-risk crops. Developing farmer cooperation and the correct economic pressures is an alternative approach to crop restrictions.
Farmers want the greatest economic advantage possible. Vegetable and other high-risk crops often present large economic returns, especially if production is done close to urban centres. These are often high-disease risk areas because of urban wastewater disposal practices. There is mounting evidence that the use of contaminated water to irrigate vegetables and certain fruits near urban areas is one of the chief means of gastrointestinal disease spread including cholera (Shuval, 1993). This has increased the urgency for health officials to restrict such production in these heavily contaminated areas. This in turn has increased the urgency to use economic incentives to develop safe production areas to meet national production needs.
Developing a programme to promote safe production areas is an alternative to crop restrictions and can be done with a three-phased process. Each phase depends upon the successful completion of the previous phase. The first phase is to develop a sound information base (water quality monitoring phase) that can be used to evaluate the existing levels of contamination (water quality) in the water being used for production. This phase includes selection of contamination indicators, establishing field sampling methods, defining acceptable laboratory analytical methods, selecting participating laboratories, selecting the field monitoring sites and conducting a field water quality monitoring programme. A full discussion of these steps is in the following section of this report.
The second phase involves evaluating the water quality data collected in the first phase and developing procedures to assess the levels of contamination (data analysis and evaluation phase). The overall goal of both Phases I and II is to ensure that the database can be used to define safe production areas. The database could also be used as a basis to control or regulate contaminated water use in vegetable or other high-risk production areas. This phase includes developing a reliable database and retrieval system, defining the methods used for analysis of the data and development of techniques that could be used to present the data. A discussion of the options available for analysis of the results is in the section Organization and analysis of the information of this chapter.
The third and final phase of developing a programme to promote safe production areas is developing mechanisms to regulate the use of contaminated water on vegetable or other high-risk crops (crop or water certification phase). The focus here is to look at options other than crop restrictions that can be used to promote safe production areas. The discussion of certification programmes along with the range of policy options that needs to be considered are found in the sections Certification programmes and Institutional and policy issues of this chapter.
Phase I: Development of a sound information base
Selection of study areas to determine safe production zones
Parameters used for measurement of microbial contamination
Analytical methods which can give acceptable results
Laboratory selection
Field sampling techniques
Criteria used to select field monitoring sites
There is a need for an improved understanding of the extent of irrigation water contamination; therefore the first phase of any effort to define safe production areas should be to develop a programme to evaluate the quality of water being used for irrigation, both on a regional scale and on a production level.
The goal of a water quality monitoring programme is to determine how extensive the irrigation water contamination is and at what level. Five principles should guide development of the water quality monitoring/evaluation programme:
In order to conduct a monitoring programme, several steps are necessary. The following sections discuss the principles involved in implementing each of these steps:
Selection of study areas to determine safe production zones
Water quality sampling for microbial contamination is normally conducted by the health authorities. Their preliminary data often show extensive contamination of irrigation water supplies especially in developing countries (Table 6). The dilemma for the agricultural and health authorities is that the problem is often too widespread for the monitoring resources available, therefore priorities must be established on where data should be developed for decision making. Health and agricultural authorities often have different priorities. To help the agricultural authorities establish priorities and choose monitoring areas, the following factors should be considered:
· whether a crop quarantine programme is planned or underway in that region of the country;
· extent of production of crops identified by the health authorities as high-risk crops;
· extent of total vegetable production in that region of the country (all vegetable crops irrigated with contaminated water pose a high risk either from consumption or via worker health and safety); and
· the likelihood that vegetable production would shift to a new area as further quarantines are implemented.
sampled 2-3 times. Based on this information then choices of irrigation systems to be sampled can be made.
The laboratory needs to be able to operate on a seasonal basis because of the nature of the irrigation season. Testing during the irrigation season requires a large effort for only a 3 to 6 month period. One alternative approach is to utilize laboratories of the Ministry of Public Health and the universities. The decision to utilize such labs is often made because of cost, availability of analytical space and lack of a quality control programme for private laboratories in the country. In addition, the use of Public Health and university laboratories gives a higher level of creditability to the results over those that would be obtained through private laboratories. This is an important consideration when the information developed may be used to plan or restrict cropping patterns.
Irrigation water contamination is a public health issue and it is valuable to have the expertise of the public health authorities or the university in interpreting results that a private laboratory would not have the capability to do within a reasonable cost. This is especially important because the laboratory will be working with two factors which are not routine in most private laboratories: contaminated water and irrigation water.
The use of public health authority and university laboratories is recommended for the initial assessment but the private sector must be capable of carrying on a long-term programme or the Agricultural Ministry will find it necessary to build up this expertise. The analytical and operational costs will likely be higher in any public sector effort, and the Agricultural Ministry assumes a substantial risk to its reputation if analyses are not done correctly or are misinterpreted.
FIGURE 5: Division of a typical river basin into reaches based upon the location of major urban wastewater discharges
CONTROL OF IRRIGATION WATER USED IN PRODUCTION OF VEGETABLE CROPS
A programme to control vegetable production using a 3-phased approach was tried in Chile in 1992. Given below is the work plan from this project.
A cholera outbreak and the high level of gastrointestinal diseases in Chile occurred because contaminated water was used to irrigate vegetables that are eaten raw (Shuval, 1993). Until a few years ago there was no programme to identify areas with high levels of contamination or to regulate production. Initial steps by Servicio Agrícola y Ganadero (SAG) of the Ministry of Agriculture and the Ministry of Health to control production in the worst contaminated areas significantly reduced disease transmission. There was, however, an urgent need for SAG to identify the level of contamination in other areas of the country and to develop programmes that promote or certify production in the safe areas to maintain national production needs.
SAG and FAO developed a one-year project with the goal of defining the levels of contamination, preventing further contamination and, where needed, developing the procedures to regulate the use of the irrigation supply water to attain vegetable and fruit production that met the sanitary protection requirements for marketing at the national and international level. The work plan for the project was to:
(a) develop a plan for systematic monitoring of the principal irrigation canals in selected zones in the country. This plan was to include the selection of indicators of contamination, field and laboratory methods to determine the values, and the logistics of conducting the plan. To establish this plan, the project needed to define the cost of operation, probable duration, the benefits and the ways to finance the programme;
(b) establish a computer database with the needed systems of quality control for processing the raw data and define how the data should be analysed and presented. This database was to include both historical data and data generated in this project;
(c) define a practical method to identify the geographic extent of contamination and define the priority for action to regulate cropping that uses contaminated water;
(d) study the options for a programme of crop certification based on the level of contamination by district or irrigated region with the goal to reduce the spread of gastrointestinal diseases caused by contaminated vegetable products;
(e) define an agreement for a crop certification programme with a system of regulations that conforms to the nature and levels of contamination;
(f) define the practical standards for laboratories and their quality control that are applicable to determining irrigation water contamination with particular attention to the agents that cause the gastrointestinal diseases; and
(g) define the basic elements of a national strategy to contain the origin of contamination of water used in agriculture.
The work plan for the project had a variety of elements (described above) and conducting all simultaneously was beyond the resources of the project. It was decided that the project should be operated in three phases. The first phase was to develop a sound information base (water quality monitoring phase). The second phase was to evaluate the data collected and assess the levels of contamination (organization and analysis of the information). The third phase was to develop mechanisms to regulate cropping and use of contaminated water on vegetable crops (crop certification phase). Each phase of the project depended upon the successful completion of the previous one. The project counterpart was the Department of Natural Resource Protection (DEPROREN) of SAG along with strong assistance from DEPROREN in Quillota (V Region) and during the third phase on crop and water certification from the Department of Agricultural Protection of SAG.
This project needed to consider a variety of issues from public health to irrigation systems and wastewater treatment. No one group, nor the project, could successfully complete the work plan without strong assistance and advice from a number of agencies. These agencies met periodically to review project results and offer guidance to the project team. Those agencies included: Dirección General de Aguas del Ministerio de Obras Publicas; Ministerio de Salud; Instituto de Salud Pública (Metropolitan and V Regions); Empresa Metropolitana de Obras Sanitarias (EMOS); Empresa de Servicios Sanitarios de Valparaiso (ESVAL); Departamento de Riego del Ministerio de Obras Publicas; Instituto de Investigaciones Agropecuarias (INIA); and Instituto de Desarrallo Agropecuario (INDAP). At the conclusion of the project a workshop was held on the project results along with the activities of the other agencies.
The project had a full-time national irrigation expert who was responsible for the daily operation of each of the project phases. The project was also supported by international consultants in water quality, crop certification, laboratory methods for microbial testing of water and laboratory methods for microbial testing of food.
(Source: FAO, 1993)
USING CROPPING DATA TO ESTABLISH MONITORING PRIORITIES
Data on vegetable production is normally available through the Ministry of Agriculture. An example of utilizing cropping data is illustrated by utilizing data from Chile. Ministry of Agriculture data on vegetable production show the most important areas in the country were the Metropolitan and the V, VI and VII Regions (see Table 9). In this case, the Ministry of Health began a crop restriction programme for 14 high-risk crops in the worst contaminated areas of the Metropolitan Region (MR).
TABLE 9: Estimated surface area of total vegetable production in Chile by region 1991-92 (Source: FAO, 1983)
Production by region
REGION
ha
I
II
III
IV
V
MR
VI
VII
VIII
IX
X
XI
XII
TOTAL
Since the quarantine only applied in the MR, evaluating other areas had to be based upon other factors. One of the most important factors used was production of the 14 high-risk crops identified in the Metropolitan Region as compared to the total extent of vegetable production in that region. Table 10 gives the production figures for 1991-92 for 12 of the 14 crops considered to be high risk. In addition, it was considered important to recognize that production increases were occurring in the two regions (V and VI) immediately north and south of the Metropolitan Region as a result of the quarantine.
TABLE 10: Surface area of 12 high-risk crops by regions in Chile (1991-92) (Source: FAO, 1993)
SPECIES
HECTARES BY REGION
IV
V
MR
VI
VII
VII
Total
Achicoria (Chicory)
Apio (Celery)
Cilantro (Cilantro)
Endibia (Endive)
Espinaca (Spinach)
Lechuga (Lettuce)
Perejil (Parsley)
Rabanito (Small Red Radish)
Rábano (White Radish)
Radicchio
Repollo (Cabbage)
Zanahoria (Carrot)
TOTAL
Parameters used for measurement of microbial contamination
One goal of a water quality monitoring programme that measures the health risk of a water used for irrigation is to identify the extent of microbial contamination, especially human.
CHOOSING MONITORING SITES IN CHILE
Because of the remote nature of many sampling locations along rivers and in irrigated zones, the number of sampling sites may need to be limited in order to accomplish the required programme. For example, in Chile, based on resources available to the Agricultural Ministry, an initial assessment of the extent of microbial contamination was first conducted nationwide. Based on these results, it was decided to concentrate efforts in the Metropolitan and V Regions for a pilot programme. This area represented 50% of the population (Table 11) and 49% of the total vegetable production in the country (Table 12.). In addition, the two Regions contain 80% of the total production in the country of the 14 high-risk vegetable crops identified by the Ministry of Health (Table 10). Crops restrictions were also being enforced in part of this area and the laboratories in these regions had analytical capacity and experience working with irrigation water.
TABLE 11: Population census in 1992 for the Metropolitan and V Regions as compared to the national total for Chile (Source: FAO, 1993)
Region
Population
V
Metropolitan
National
TABLE 12: Percentage of total vegetable production that occurs in the Metropolitan and V Regions (1992-92) (Source: FAO, 1993)
Region
Hectares
V
Metropolitan
National
Several indicators can be used and each has advantages in particular situations. One of the weaknesses of most indicators is that there is little understanding of the relationship of actual disease to any of the indicator levels. Such information is extremely difficult and costly to develop.
In choosing a water quality indicator several factors must be considered:
Various WHO epidemiological studies show that waste-related diseases are very common in developing countries. Many of these diseases are predominant in wastewater and pose a significant potential risk to public health during wastewater use by irrigated agriculture. The conclusion of these epidemiological reviews is that when untreated or partially treated wastewater is used to irrigate crops there is a higher health risk from intestinal nematodes and bacteria than from viruses and protozoa (WHO, 1989).
As discussed is Chapter 2, in most developing countries untreated wastewater is often diluted in surface water supplies and then used for irrigation. There is no evidence to indicate that the ranking of the high-risk pathogens would be significantly altered by this dilution.
Results of epidemiological studies place the highest risk from nematodes in the irrigation water and the next greatest risk from bacteria. Therefore, any monitoring parameters or water quality indicators should focus on these two. The WHO Guidelines described in Chapter 2 evaluated the risk potential of the various pathogen groups and established safe levels (Table 5). From these guidelines, the water quality indicator for nematodes is the helminth eggs while for the bacteria the water quality indicator is faecal coliform (WHO, 1989).
Helminth egg indicator
The helminth egg indicator refers to a group of intestinal nematodes and is not specific to one, thus it covers a range of potential diseases. However, helminth organisms are not endemic in all areas and constant monitoring in some areas may not be needed (WHO, 1989; Shuval et al., 1986). Some of the most important are Ascaris, Trichuris and an number of hookworms. The helminth egg water quality indicator appears to have gained wide international acceptance (WHO, 1989; FAO, 1992; Shuval et al., 1986b) and now forms a basis for guidelines on wastewater treatment efficiency. National acceptance has not been fully established as the recent WHO Guidelines (WHO, 1989) are only now being evaluated and considered for adoption as national health standards in developing countries. As a result, although gaining in recognition and acceptance, there is little experience in applying helminth egg organisms as a water quality indicator or as a measure of the efficiency of treatment.
Because the WHO helminth egg indicator is just now being reviewed and considered in many developing and developed countries there is a lack of experience in how to monitor for it and the field monitoring techniques are not well established. WHO (1989) recognized this by stating:
"The helminth egg guideline value is intended as a design goal for wastewater treatment systems, and not as a standard requiring routine testing of effluent quality. The most sensitive techniques currently available for the detection of helminth eggs in wastewater are able to detect a minimum of the order of one egg per litre. However, these are not practicable for field monitoring purposes."
As a result of limited experience in developing countries using the helminth egg standard, there is likely to be little experience by national laboratories in doing routine analytical procedures. The only likely experience in the short-term will be in university laboratories or in laboratories established for monitoring the process control at the municipal wastewater treatment plants. While these laboratories can be utilized to assist in special studies or for quality control and quality assurance, it is unlikely that these laboratories would be available for routine analyses.
In summary, for an initial assessment of irrigation water quality at the field or place of use, using the helminth egg indicator is not practical at the present time. Any detection of high helminth egg levels is likely to occur in conjunction with high bacterial populations both of which present a serious threat to high-risk crops such as vegetables. Therefore it appears more practical to place the short-term emphasis on defining useful bacterial indicators.
Bacterial indicators
The coliform group of bacterial organisms has been used as an indicator of faecal pollution. The sources of coliform can be faeces of warm-blooded animals, but can also be vegetation and soil. The use of total coliform levels is not recommended as their source can be other than faecal contamination. Specific examinations would be needed to confirm faecal contamination. High total coliform levels, however, should be a strong indicator of possible faecal contamination in irrigation water.
A standard test for the coliform bacterial group is their ability to ferment lactose at 35°-37°C. This is a "presumptive test" for faecal pollution since no actual examination is made during the test for the faecal coliform group. Faecal coliform organisms are different from other members of the coliform group because they are thermo-tolerant organisms. This thermo-tolerant characteristic has often been used to isolate faecal coliforms from other types of coliform organisms. For example, at a temperature near 44 °C, faecal coliforms will continue to ferment lactose and those of soil and vegetation will die.
In temperate climates faecal coliform organisms are mostly Escherichia coli (E. coli) which are always found in the faeces of man, animals and birds in large numbers but rarely found in natural water not subject to pollution. The presence of E. coli is regarded as definite proof of faecal pollution.
Because Escherichia coli is the predominant bacterial species in most human faecal material, its count is often cited as the most reliable indicator of human waste. This indicator, however, has not been used in international or most national standards to assess the usability of irrigation water or for process control on wastewater treatment plants. While the method is understood by most university and Health Ministry laboratories, most private laboratories do not conduct this test on a routine basis. There are efforts underway to work this indicator into national and international standards, however, it would be premature to utilize this indicator for irrigation water without a basis to assess the results. WHO Guidelines and national regulations at this time continue to emphasize the use of faecal coliforms as a universal indicator.
A disadvantage to the faecal coliform measurement is that faecal coliforms occur in both human and animal sources of pollution and detection does not tell whether the water contamination is of human or animal origin. One method to help define the source is to use the faecal streptococcus group which also occurs in all faecal material. Definite ratios of faecal coliform to faecal streptococcus have been established to define the differences between human and animal sources of pollution. This test should be used with caution. There are no data available to show a strong relationship between this ratio or the faecal streptococcus level and disease transmission from vegetables eaten raw or other crops. In addition, it is difficult to see the ratios when mixed pollution sources are present or if there is a move downstream from the pollution source because of differences in die-off rates. Because of these limitations and the vastness of normal irrigation networks, use of faecal
streptococcus as a routine indicator is not recommended. Its use should be considered when looking at secondary sources of contamination within an irrigation system.
From the discussion above, there are several tests available to determine the degree of risk. From a regulatory point of view, the recommended indicator must be based on strong scientific evidence. In addition, because a field monitoring programme is being conducted, any evaluation must be with a known indicator that gives the highest potential to represent the present disease risk, but the indicator must be within the capability of developing countries to interpret and utilize the information developed. With the limitations of monitoring for helminth egg levels in irrigation water and because gastrointestinal diseases including cholera are still a major focus, faecal coliforms are recommended for use as the indicator. Faecal coliform measurements are the most common and practical indicator presently available for determining the degree of disease hazard caused by pathogen occurrence in irrigation water used on fruits and vegetables. This indicator represents the best
scientific information presently available to assess the primary objective: protection of public health (WHO, 1989).
Faecal coliform organisms as an indicator are recognized internationally by WHO and the Pan American Health Organization (PAHO). Faecal coliforms are used as a primary indicator in the United States, Canada, Europe and many developing countries. Faecal coliforms are also used in many national regulations that deal with wastewater and irrigation of crops, which makes the transfer of data and experiences easier. Faecal coliforms are within the means of public and private laboratories to make determinations on a routine basis.
It is also recognized that using faecal coliforms alone as the only indicator presents a risk. The guidelines of WHO (1989) also call for the use of helminth egg level, but as described above its use for routine monitoring is not practical at the present time. It is also recognized that the WHO Guidelines were developed for process control on wastewater treatment facilities and it is unknown whether dilution of untreated or partially treated water will present the same health risk as treated water. Using only the faecal coliform indicator assumes a risk, but it is likely this risk is minor when compared to the present level of irrigation water contamination.
The WHO bacterial guideline for unrestricted irrigation use is described as less than or equal to 1000 faecal coliforms per 100 ml measured as a geometric mean of the samples taken. As previously discussed, this guideline was developed as a performance goal for construction and operation of wastewater treatment plants. Its original intent was not to be used in routine monitoring, although, as discussed in Chapter 2, the same standard has been used for an irrigation water guideline in several countries. There is little experience using it under field conditions. Because of variability under field conditions, there is concern that there may be considerable variability in the data. To ensure uniformity in applying the guideline, it is recommended that the following criteria be used when monitoring agricultural supply water:
In evaluating the guideline, the geometric mean of all samples taken during the irrigation season shall be less than or equal to 1000 faecal coliforms per 100 ml and that only 10 percent of the samples can exceed the guideline value. If less than 10 samples are taken, then not greater than 1 in 5 can exceed this guideline. The exception is that not greater than 20 percent of the samples can exceed the guideline (not greater than 2 in 5 if less than 10 samples are taken) if no one sample exceeds twice the guideline value.
This approach to measuring compliance uses similar methods to those used by the US Environmental Protection Agency (US EPA) to measure the bacterial safety of swimming waters and those used for other recreational purposes. The exception to the guideline is not utilized by the US EPA but is included here to allow more flexibility is using wastewaters for irrigation while over the entire irrigation season maintaining a safe bacterial environment.
CHOOSING AN INDICATOR OF CONTAMINATION
The choice of a contamination indicator must be within the national means to measure and assess. For example, a project in Chile (1992) chose faecal coliform organisms as the primary indicator. Because gastrointestinal diseases including cholera were the main focus in Chile, the indicator of faecal coliform was recommended for use. This indicator is recognized internationally by WHO, PAHO US environmental Protection Agency (EPA). Faecal coliform is also used as a primary indicator within all Latin American countries which makes the transfer of data and experiences easier.
Faecal coliform was within the means of public and private Chilean laboratories to make determinations on a routine basis. Faecal coliform is also used in regulations in Chile that deal with wastewater and how it can be used for irrigation of crops (NCh 1333, dated 1978). In addition, the Health Ministry issued a detailed description of how the data will be evaluated for regulation purposes (N° 0350, dated Jan 1983).
Escherichia coli (E. coli) was not used because it had not been used in either international or Chilean standards to assess the usability of irrigation water. While the method was understood by the university and Health Ministry laboratories, most private laboratories in Chile do not conduct this test on a routine basis. The Chilean regulation and WHO standards at this time continue to emphasize the use of faecal coliform as a universal indicator.
The standard defined in Chilean Norm 1333, dated 1978, prohibits production of certain vegetables when the monthly mean of 5 samples or more shows faecal coliform levels above 1000 per 100 ml. This is the same level recommended by the US EPA for irrigation waters. The level of less than 1000 was also recommended by WHO (1989) as a design standard for wastewater treatment plants but it was felt that using the standard in this programme as an irrigation water quality regulation was a good starting point to lower the present health risk from unrestricted irrigation.
It is important to note that the WHO Guidelines call for a period of measurement "during the irrigation period" (WHO, 1989). The US EPA guidelines and those used in many developing countries are based upon a monthly or 30-day period. The WHO Guideline is for wastewater treatment plant design while the 30-day period used elsewhere is used for monitoring in-stream water quality. The logic behind the 30-day period is to be able to respond to poor water quality and make changes. Such a response system is not as critical for irrigation water quality because values slightly higher than the guideline do not imply that a disease outbreak will occur. For monitoring irrigation water quality, the 30-day or monthly geometric values should be given lesser importance than to gather a set of data that represents the quality used throughout the irrigation season.
Analytical methods which can give acceptable results
There are several methods to determine faecal coliform. These methods were developed principally for testing drinking water and wastewater. There is little experience developed specifically with irrigation water supplies but the present techniques should be adaptable to irrigation water. In choosing an analytical method, the following should be considered:
There are two basic methods that are widely used for determining faecal coliform. The first is the multiple-tube-fermentation technique or sometimes called the most-probable-number (MPN) technique. The second is using the membrane filtration (MF) technique. Both techniques are described in Standard Methods for Examination of Waters and Wastewaters (APHA, 1992).
In both techniques the faecal coliform organisms are separated from those of non-faecal origin by using elevated temperatures. The standard multiple-tube-fermentation technique uses a two-step procedure. The first step is a "presumptive test" for the presence of coliform organisms but these may be of both faecal and non-faecal origin. This step only shows that coliforms are present and does not differentiate the type of coliform. A positive in this stage of the test is based on the ability of the coliform organisms in a tube to ferment lactose broth and produce a gas.
The second step of the procedure then transfers coliform organisms from all the positive tubes to a different growth medium and these are kept at a higher temperature. Faecal coliforms will again produce an amount of gas while other coliforms will die at the elevated temperature. This is called a "confirmed test" for faecal coliforms. This standard two-step procedure requires 48-72 hours to complete. If a large number of samples are being collected this can tie up a tremendous amount of laboratory staff time, equipment and space.
There is a more rapid "confirmed test" which is a modification of the two-step procedure. This newer one-step method uses an A-1 medium. This test requires only 24 hours to complete and many laboratories already use it. It is in Standard Methods (APHA, 1992) and significantly reduces the amount of time needed to confirm the presence of faecal coliforms.
The second method used to determine faecal coliforms is the membrane filtration technique (MF). The technique involves passing a known volume of water sample through a membrane filter that has a very small pore size. The bacteria are retained on the filter because they are larger than the pores. The filter is then put on a medium for growth of faecal coliforms. After 24 hours, the faecal coliform colonies can be counted and the concentration in the original water sample determined. The MF technique is faster than the MPN procedure and gives a direct count of the number of faecal coliforms while the MPN technique is only a statistical estimate of a certain concentration being present.
The MF technique was primarily developed for rapid determination of low faecal coliform levels in clean drinking water supplies. Contaminated irrigation water samples are likely to contain high sediment levels, high organic load, high nutrient levels and high faecal coliform levels. The MF technique may develop problems with the high sediment and organic load in contaminated irrigation water. These may be overcome by the extensive dilutions that would be needed to allow accurate colony counts; however there are few data to support this assumption.
USING FAECAL COLIFORMS TO ASSESS IRRIGATION WATER CONTAMINATION IN CHILE
An example comes from an irrigation water assessment project in Chile. In the Metropolitan Region (MR) surrounding Santiago, water samples contained high sediment levels, high organic load, high nutrient levels and high faecal conform levels. Based on these constraints, the only practical method of analysis was the multiple-tube-fermentation technique (MPN). The alternative technique, the membrane filtration (MF) technique, is likely to have problems with the high sediment and organic load. In addition, dilutions would need to be extensive to allow accurate colony counts.
In Region V the level of contamination in the irrigation supply channels is not as severe as in the Metropolitan Region of Santiago. The irrigation supply channels do contain sediment and in some instance have strongly elevated levels of faecal conforms. As in the MR, these factors placed a severe constraint on use of the MF technique. Data from the University of Chile show the MF technique had problems and supported the conclusion that the multiple-tube technique was the only reliable one.
The project needed to complete many samples in a limited period of time. The project used the rapid confirmed test which requires only 24 hours to complete. This modification of the MPN technique used the A-1 medium. Although this test is not in the Chilean regulations, it is in Standard Methods (APHA, 1992) and significantly reduced the amount of time needed to confirm the presence of faecal coliforms.
The question arose whether the data developed by the project were valid because the A-1 medium was used. Many laboratories in the country already used the method and tests conducted at the University of Chile prior to the project as well as other international research showed the A-1 method to be equivalent to the full two-step method for a full range of Chilean or similar waters. The data developed by the project therefore were not suspect. The project concentrated on obtaining the faecal coliform data with the least cost approach to the multiple-tube fermentation technique.
It is recommended to use the multiple-tube-fermentation technique (MPN) for analysis. If possible, the membrane filtration technique (MF) should be tried also as it leads to greater flexibility in sample collections, location of sample collection and number of samples that can be handled (see discussion in the section Field sampling techniques). Regardless of the procedure used to determine the faecal coliform levels, both are an accurate estimate of the probable concentration therefore the data from either technique are valid.
In using the MPN technique to analyse contaminated irrigation water supply samples, the laboratory can utilize either the standard two-step multiple tube fermentation technique or the one-step method that utilizes the A-1 medium. Neither have been widely used for irrigation water applications but unless there is research evidence in the country that shows a difference, the two-step and the A-1 medium should be equivalent. It is also not likely that the A-1 medium procedure is encoded in national regulations as it is a relatively recent technique. Many laboratories probably already use it because it is in Standard Methods (APHA, 1992) and is recommended by WHO (Mara and Cairncross, 1989).
Laboratory selection
The selection of laboratories to assist any monitoring project should be based on three criteria:
All three of these factors are likely to place major constraints on how extensive the monitoring programme can be. The availability of reliable laboratories in developing countries must be considered in planning a programme to promote safe production.
Normally, laboratories within the Agricultural Ministry emphasize agricultural production needs and not public health issues. In most instances, they will not be equipped nor are staff adequately trained to produce a high degree of credibility on public health-type analyses.
The alternative is to strengthen the quality of private laboratories. This requires that a programme of laboratory testing and certification be developed to ensure that the private laboratories can conduct accurate and reproducible faecal coliform testing procedures. The Agricultural Ministry, however, should not become involved in certifying such laboratory performance. Microbial testing for disease potential is beyond the expertise of the Agricultural Ministry and should be left to the Health Ministry, the university or other responsible public health organization.
The Agricultural Ministry, in operating a programme to promote safe production areas, needs to have the capability to do random compliance monitoring. Developing that laboratory capability in-house means there must be sufficient expertise to conduct analyses to a high degree of credibility during the relatively short irrigation season. The actual cost per sample may be quite high under such conditions. An alternative approach is to seek cooperators such as the Health Ministry or the university. Both of these groups can do quality work on a large number of samples but more important they bring a high level of credibility to the analytical work. This removes any doubt when analyses are used for difficult decisions such as defining an area safe or not certifying an area.
Field sampling techniques
A full description of the field sampling techniques is given in Standard Methods for Examination of Waters and Wastewaters (APHA, 1992). As described in Standard Methods, field sampling is subject to many sources of contamination and must be done with care. Because even the slightest contamination can affect sample results, it is recommended that all samples be collected by the laboratory or other professional staff that are trained in bacterial sample collection.
A few of the key points to remember in field sampling are:
SAMPLING TECHNIQUES CAN AVOID CONTAMINATION
For example, sampling from a bridge may require a sampling device. A sampling pole with the bottle attached is the preferred approach. Often the sample is collected in a bucket and then a sample taken from the bucket. In instances where a bucket is needed, it must be remembered that the bucket represents a serious source of contamination if it has not been thoroughly cleaned after each sampling site. A thorough cleaning would mean use of a disinfectant such as soap and rinsing at least 3 times with the water to be sampled prior to its use for sampling.
During development of a bacteriological sampling programme there are several points that should be considered because they limit the number of samples collected and raise the cost of sample collection:
FIGURE 6: Typical sampling schedules to accomplish five samples within 30 days
a) Laboratory Available 5 Days Per Week
Sun
Mon
Tue
Wed
Thurs
Fri
Sat
Zone 1
Zone 2
Zone 3
Zone 1
Zone 3
Zone 2
Zone 3
Zone 1
Zone 2
Zone 2
Zone 3
Zone 1
Zone 2
Zone 3
Zone 1
b) Laboratory Available Only 4 Days Per Week
Sun
Mon
Tue
Wed
Thurs
Fri
Sat
Zone 1
Zone 2
Zone 3
Zone 2
Zone 3
Zone 1
Zone 1
Zone 3
Zone 2
Zone 3
Zone 2
Zone 1
Zone 3
Zone 1
Zone 2
It is not recommended to carry more than three irrigated zones during any sampling cycle as samplers loose track of established sites.
The constraints listed above mean that sample collectors are often utilized for only partial days and often not on Fridays. As a result, the total cost per sample collected is extremely high unless these people are trained for other work. These constraints may also limit the ability to sample extensively over large areas or far from established laboratories.
The constraint of not having access to laboratories or attempting to sample in remote areas will seriously hamper the widespread appraisal of safe production areas. This constraint could be overcome if the membrane filter (MF) technique could be adopted for use on irrigation water. This would enable the samplers to use portable equipment which would allow greater flexibility in scheduling and locating sites.
If a programme to develop safe production areas is being proposed, it is recommended that the Agricultural Ministry, through its research branch, cooperate jointly with the Health Ministry and the university to evaluate the use of the MF technique for irrigation water and whether portable equipment could be developed for use in that country. The use of the MF technique could greatly expand the ability of the Agricultural and Health Ministries to promote safe production areas and eliminate production areas that have a significant health threat. The analytical cost per sample may be more for the MF technique but this would be offset by reduced staff time for sample collection and transport.
Criteria used to select field monitoring sites
The first step in selecting water quality monitoring sites is to divide the area into logical irrigated zones that are served by a common water source. The only consideration should be hydrology; the size of the area served should not be considered at this time.
The next step is to consider the contamination sources that would influence bacterial quality of the irrigation water. The two most important contamination sources are:
In establishing monitoring points, the programme needs to consider the external influences (primary contamination) as a first priority. The first monitoring points are set based upon preliminary data that are available to the project which show how extensively discharges to the rivers impact or influence irrigation water quality. Monitoring points are then set to assess the actual influence of these discharges. In instances where data are not available for this preliminary assessment, water quality sampling will be necessary to establish the extent of supply or river water contamination (see discussion section Selection of study areas to determine safe production zones).
Based upon a preliminary assessment of primary contamination, a decision tree needs to be developed to determine if further sampling should be conducted to evaluate the extent of secondary contamination. A recommended decision tree is shown in Figure 7. The overall goal of the decision tree is to limit expenditure of resources and focus on monitoring those areas that the Agricultural Ministry could promote as clean production areas.
The decision tree was developed to conform with the WHO bacterial Guidelines (WHO, 1989). The decision tree in Figure 7 shows how these guidelines were slightly modified to reflect the variability that occurs under field conditions (see discussion in section Bacterial indicator). The decision tree also reflects the need to identify both the lowest and highest risk areas without strict adherence to any established guidelines. This preliminary phase of monitoring is still far removed from the actual irrigation use and is only attempting to define areas with the greatest potential for being used as a safe production area.
The decision tree (Figure 7) is the initial step of an area-wide water quality evaluation and starts with the water source not the actual point of use. During this initial assessment, it is recommended that 10 000 faecal coliforms per 100 ml (10^4) be used as a decision point rather than 1000 faecal coliforms per 100 ml (10^3) as specified in the WHO Guidelines. One of the primary reasons for this recommendation is that areas with water supplies < 10^4 have the greatest potential to reduce the contamination to a level that would make this a safe production area in the future. It must be remembered that this is an initial area-wide assessment and may still be several kilometres from the actual point of use and some natural treatment and coliform die-off may occur.
This approach evaluates a broader area. The selection of the 10^4 rather than 10^3 is not based upon the intent to certify these areas or imply that they meet health guidelines; this is used only as a point of decision making for choosing additional sampling sites. The decision tree (Figure 7) was built upon the following three goals:
Using the decision tree allows a determination of which areas are potentially clean and would be worth spending additional resources to promote safe production. The decision to stop further sampling after samples showed >10^4 faecal coliforms per 100 ml was based upon experience gained in an irrigation water evaluation in Chile which showed these highly contaminated areas remain highly contaminated throughout the entire irrigation system (Figure 8) (FAO, 1993). These high levels also pose a significant danger to public health and worker safety in the rural areas.
Once it has been determined that an area has a source water that is <10^4 faecal coliforms/100 ml, the extent of secondary contamination or contamination that occurs within the irrigation system should be determined. Secondary contamination will be an important factor in the quality of water used in vegetable production. The principal reason is that vegetable production in most developing countries occurs on small farms in densely populated rural areas. With a lack of adequate rural sanitation, the canal water supply is frequently used as a disposal point. Because rural villages often congregate near supply canals, the secondary discharges can be significant.
FIGURE 7: Criteria used for taking initial water quality samples for faecal coliform (FC) in irrigation water
FIGURE 8: Extent of faecal contamination in the irrigation water of the Metropolitan Region of Chile within the irrigation system as compared to the initial level of contamination in the source of water
(Source: FAO, 1993)
Figure 8 shows that in the Metropolitan Region surrounding Santiago, Chile, almost 40 percent of the irrigated area was served by river or groundwater supply that met the WHO Guidelines of less than 1000 faecal coliforms per 100 ml, yet after sampling within the irrigation system less than 10 percent of the irrigated area still remained safe for vegetable production. The reason for the loss of safe areas was the result of secondary discharges into the irrigation supply system after the water was diverted from the river.
Sampling for secondary contamination within an irrigation system can be time-consuming and costly. The approach recommended is to assume that an irrigated zone is the focus for developing a safe production area and not individual channels or the individual fields. To assess secondary contamination, sampling points need to be chosen within the system. The geometric mean can be determined from multiple samples from an individual site or from single samples from multiple sites within the irrigation network. The criterion for selecting how to determine the geometric mean is based on the assumption that the individual channel or group of channels represents similar channels in the irrigation network. The goal is to identify whether secondary contamination is a significant factor in the irrigated zone but without monitoring the quality of water used in each individual field. The choice of sampling points is extremely important. The following factors need to be considered:
The final factor listed above implies that a minimum size area be established. This is extremely difficult to determine and needs to be a judgement factor based on cropping patterns, water use, contamination potential as well as previous experience. As a guideline, the minimum area for monitoring secondary contamination should not exceed 1000-2000 hectares until sufficient experience is gained in sampling irrigation water quality for secondary contamination.
Phase II: Organization and analysis of the information
Historical sources of data
Sampling frequency and criteria used to evaluate the data
Database, mapping systems
Historical sources of data
Existing water quality data can be an important way to assess how widespread contamination is. Previous monitoring programmes are usually not sufficient to understand the extent of contamination or provide a sound basis for decision making. These data, however, provide an initial assessment of where more intensive monitoring should be conducted.
There are several potential sources of information on reconnaissance level monitoring for bacterial quality:
A. River Water Quality Studies
1. Natural Resource Departments
2. National Water Authorities
3. National Irrigation Departments
4. Drinking Water Supply Authorities
5. University Studies
B. River Contamination Studies
1. Wastewater Treatment Authorities
2. National Public Health Authorities
3. Municipal Water Supply Authorities
C. Irrigation Canal Studies
1. Regional and National Irrigation Departments
2. Irrigation or Canal Associations
3. Agricultural Ministry
4. Agricultural Research Organizations
5. Regional Health Services
6. Local Water Supply and Sanitation Groups
USING HISTORICAL INFORMATION
An example of how historical information can be useful to a project is shown from an FAO project in Chile (1992).
In the Metropolitan Region surrounding Santiago, Chile, the historical information was principally from sampling of the Mapocho River and the Zanjon de la Aguada. This information showed very high contamination levels in the river and permitted the FAO project to infer that canals taking supplies from these sources would be equally contaminated. These studies were conducted by the Department of Sanitary Engineering of the University of Chile and by the Direction General of Water of the Ministry of Public Works.
After the outbreak of cholera in Chile, the Department of Natural Resource Protection (DEPROREN) of the Ministry of Agriculture conducted a sampling of the principal irrigation canal intakes in the region. This sampling was one sample at each point which enabled the FAO project to develop a preliminary assessment of the different levels of contamination.
In the V Region, the FAO project had available a preliminary study on the levels of microbial contamination in the Aconcagua River and its main tributaries. This study was conducted for a regional sanitation plan. This study was supplemented by a preliminary evaluation of the principal irrigation canals conducted by DEPROREN (V Region) in cooperation with the Regional Health Service. Again these studies were based on only one sample at each main canal but they gave the FAO project a sound basis to develop a more extensive evaluation.
In other regions of the country DEPROREN, in cooperation with local public health services, conducted, in 1991, a preliminary sampling of the principal canals in the country. This sampling was not of the same intensity as the sampling done in the Metropolitan and V Regions but this information was available to the project and formed a sound basis for developing a nationwide assessment.
A key question with historical data is the quality of the data and whether they should be used. The guideline at the initial assessment phase of a monitoring programme is to use the data as a guide, not view the data as an absolute value that will be used for cropping decisions. Historical data should only be used for decisions regarding sampling intensity.
It is also important to have available a number of maps as these are essential to planning a sampling programme. The most important maps are:
· topographic maps (1:250 000) of the regions;
· topographic maps (1:50 000) of the river basins;
· topographic maps (1:25 000) of the irrigation zone;
· detailed irrigation system maps (1:10 000) from the Ministry of Public Works or the Irrigation Department or Local Irrigation or Canal Associations. These maps should contain complete information on the canal distribution system including if possible division at the individual fields.
· aerial photos (1:20 000) containing field information and irrigation water distribution patterns.
FIGURE 9: Wastewater flow impact on the hydrology of a typical seasonal river
Sampling frequency and criteria used to evaluate the data
As discussed earlier, it is recommended to use faecal coliforms as the indicator of faecal contamination of water used for irrigation (see section Bacterial indicator). This conforms with the WHO Guidelines and US Environmental Protection Agency Standards which describe the maximum limits of faecal coliforms for unrestricted irrigation use. The maximum limit established in these standards is 1000 faecal coliforms/100 ml of water.
To determine compliance or to evaluate the monitoring results, it is recommended to use the geometric mean of five samples taken within a 30-90 day period. The decision to take only five samples per site is to minimize cost, but still account for variability under field conditions. Larger numbers of samples should be collected if resources permit. Collection of at least five samples also allows for establishing a legal basis for decision making, since most regulations set this minimum number (see section Bacterial indicator). Established regulations often specify that all samples must be collected in a 30-day period. These established health regulations should be complied with. In the absence of established time periods, a 30-90 day period represents a typical vegetable growing cycle. The selection of an alternative time period for completing the minimum number of samples should be based on the period between irrigation applications or a series of irrigation events. In no
case should sampling outside the irrigation season be considered as acceptable evidence of no contamination.
An additional consideration in setting the sampling frequency is the hydrology of the river that supplies irrigation water. Most river flows are cyclic, often corresponding to the seasons. Wastewater flows on the other hand are relatively uniform throughout the year. This results in periods of low natural stream flow when wastewater makes up a great portion of the total flow and vice versa (Figure 9).
As discussed earlier, quality control samples should be collected as part of any routine sampling programme. At least 10 percent of the samples collected should be for quality control. Quality control samples should be a mixture of both duplicate samples and blank samples. Duplicate samples should comprise at least 80 percent of the quality control samples. Duplicate samples are samples taken from the same source at the same time. This assumes that the water source is well mixed which may or may not be true under field conditions. True duplicate samples are normally a split sample of a larger sample.
Taking duplicate samples for microbial analysis will never produce identical results. Sampling for a live organism which must live and thrive during the collection and analytical process may cause differences. In reviewing data from duplicate samples, it is important to remember that faecal coliform results are presented as the most probable number (MPN). This means that a range of values could be found. In judging duplicate samples, small differences between samples should not be judged as important as differences in orders of magnitude. For example, if the first sample is 500 faecal coliforms per 100 ml and the duplicate sample is 1000 faecal coliforms per 100 ml, this type of difference can be expected and should not be of concern because both samples are below the WHO Guideline level. Concern, however, should be expressed if the duplicate sample is >5000 faecal coliforms. In this case, the data for both samples should be discarded and new samples taken. In field testing
irrigation water supplies for faecal coliforms, if any sample exceeds the WHO Guideline and if the duplicate sample shows variability greater than 5-10X (times) the first sample, all the analyses conducted by that laboratory, on that date, should be considered suspect and new samples taken. Such a magnitude of difference would imply a laboratory error or the samples were contaminated. In instances where the difference between duplicate samples is from 2 - 5X, strong consideration should be given to additional sampling in the same area to assess the reasons for the differences in duplicate samples.
Approximately 20 percent of the quality control samples should be blank samples. These samples are from a known source of low bacteria water such as a drinking water supply. The sample is then handled in a manner similar to all field samples. The purpose of this sample is to detect any contamination that may occur in any step of the process from sample bottle sterilization to completed analytical work. When blank samples show faecal coliform concentrations greater than 2X the expected value, a review should be conducted for potential sources of contamination.
The irrigation season for an entire area is well defined even though irrigation frequency may vary. Collecting water quality data and quality control samples within a time period that corresponds to the frequency of an irrigation season or cycle permits evaluation of the data to detect erratic values. Determining erratic values for faecal coliforms is extremely difficult. The faecal coliform procedure attempts to measure live organisms that are growing and reproducing and erratic or unexplained values do occur. Resampling is needed when a value is 10X the geometric mean of the other samples. Because of the long time period needed to obtain laboratory results and analyse the data, resampling is frequently not done within a 30-day period. Being outside the 30-day period specified in public health regulations should not limit the resampling effort. The goal is to define conditions within the irrigation season and this typically extends beyond a 30-day period.
THE IRRIGATION SEASON AND DISEASE INCIDENCE
Shuval (1993) shows that the peak of the typhoid fever incidence occurs in the Santiago area of Chile following the initiation of large-scale irrigation of salad crops during the dry summer and early autumn months (November through June in the Southern Hemisphere (Figure 10)). The impact of changes in streamflow can be seen even more dramatically during the 1992 outbreak of cholera in Chile. The largest outbreaks occurred in late summer and early autumn when vegetable production was still at a maximum but natural runoff in the streams from the Andes was at a minimum. Wastewater discharges to the river were relatively constant and therefore made up a very high percentage of the water diverted for irrigation during this period.
Figure 10: Seasonal variations in typhoid fever rates in Chile (Source: Shuval et al., 1986a; Shuval, 1993)
Ranges should be established to evaluate the extent of contamination. Ranges should be based upon preliminary studies done prior to the project, initial results obtained through project sampling, and the WHO Guidelines (WHO, 1989). In the absence of preliminary information, it is recommended that four ranges be established to conduct a preliminary evaluation of the area-wide extent of contamination. These ranges can be modified as experience is gained. The first range refers to areas of bacterial contamination that are below the 1000 faecal coliforms/100 ml of water standard. These areas would be unrestricted for irrigation of vegetable crops or other high health-risk crops. The second range is between 1 000 and 10 000 faecal coliforms/100 ml (1-10X the WHO Guideline). These are generally areas found to be affected by local discharges that occur along the entire length of the canal. These areas are potentially safe production areas if the sources of the contamination can be
eliminated from the irrigation system. The third range is between 10 000 and 100 000 which shows levels of heavy contamination which will require treatment systems on the primary sources of contamination. The fourth range is values over 100 000 which show extensive heavy contamination with the majority typically being from major discharges of untreated urban wastewater.
One difficulty in interpreting data from faecal coliform measurements is that they do not differentiate whether the contamination is of human or animal origin. In large streams and rivers that show high faecal coliform counts, the contamination is most likely from human sources and in particular from wastewater discharges from urban areas. A preliminary study on the main surface water supply being diverted for an irrigated area may assume that any faecal coliform detections are from human sources.
The source of the contamination is less clear when the water supply being diverted from the river is below the WHO Guideline of 1000 faecal coliforms per 100 ml and secondary contamination into the irrigation system within rural areas increases the level above the Guideline. This is not an unfamiliar occurrence. In these instances, faecal contamination from rural areas is normally below 10X the WHO Guideline value. These areas have a high potential to lower contamination levels which would allow unrestricted crop irrigation. Determining the contamination source is crucial so that clean-up can be focused on the most important sources.
In many rural situations where human pollution is suspected on the basis of initial faecal coliform test results, the actual pollution may be caused by animal waste discharges (Geldreich, 1976). Establishing the source of pollution can be very important. Use of the faecal coliform/faecal streptococci (FC/FS) ratio is helpful in establishing the source of pollution in rural areas (see section Bacterial indicator for a discussion of faecal streptococci).
Research has shown that the quantities of faecal coliforms and faecal streptococci that are discharged by humans are significantly different from the quantities discharged by animals. Research suggests that the ratio of the faecal coliform (FC) count to the faecal streptococci (FS) count in a sample can be used to show whether the suspected contamination derives from human or from animal wastes. Typical data on the ratio FC/FS for human and various animals are shown in Table 13. The FC/FS ratio for domestic animals is typically less than 1.0, whereas the ratio for humans is more than 4.0.
A major difficulty occurs if ratios are obtained in the range of 1 to 2; interpretation is uncertain. If the sample was collected near the suspected source of pollution, the most likely interpretation is that the pollution derives equally from human and animal sources. It is difficult to see the ratio when mixed pollution sources are present or if the stream or canal is sampled too far downstream from the pollution source because of differences in die-off rates. To minimize interpretation problems, researchers suggest two important steps (Geldreich, 1976):
TABLE 13: Estimated per caput contribution of indicator micro-organisms from human beings and some animals
Animal
Average indicator number per gram of faeces
Average contribution/caput/24 hr
Ratio FC/FS
Faecal coliforms 10^6
Faecal streptococci 10^6
Faecal coliforms 10^6
Faecal streptococci 10^6
Chicken
Cow
Duck
Human
Pig
Sheep
Turkey
Source: Geldreich, 1976
Because of the concerns for obtaining a correct ratio and the vast number of sources that can discharge into an irrigation canal or river, the following are recommended guidelines:
Database, mapping systems
To organize the data collected, it may be necessary to construct a computer database. The criteria used to establish the database should be:
A monitoring project should initially use maps of the scale 1:50 000 - 100 000. These maps should focus on the main irrigation canals and the irrigated areas that are served by each of the main canals. The scale of these maps reflects the level of sampling intensity and the capability of many geographic information systems to handle and process this information in a timely fashion. The goal of a mapping effort is to develop a methodology and information tool. The maps can then be used in other areas to assess the level of contamination or be used to make cropping decisions on a national or regional basis. The map needs to be structured on the primary goal of a water quality monitoring programme; that is to identify as large an area as possible that is safe for production of high-risk crops. Mapping should not be directed at individual fields but rather defining broad areas. More detailed maps would not serve this purpose.
To develop future assessment capability, cooperation should be established with the Information Systems Group of the Agricultural Ministry or other resource agency which can digitalize the maps into a GIS. With a GIS programme, the level of contamination can be entered into the system from the monitoring database and a series of maps produced that shows the levels of contamination on a visual scale. These visual scale maps then assist planners, decision makers and field monitoring people in conducting their work or could serve as the basis for defining safe production areas. The GIS database is capable of further evaluations of the distribution and types of crops, soil production classes, urban expansion, and social and economic impacts. All of these factors need to be taken into account in a crop certification or restriction programme.
Phase III: Certification programmes and institutional and policy issues
Certification programmes
Institutional and policy issues
Certification programmes
Policy considerations for certification
Developing a water quality certification programme
In areas where wastewater is used, crop restrictions are often the focus of efforts to reduce the risk of disease outbreaks and spread. A critical factor in the success of a crop restriction programme is using the wastewater in a defined area. Crop restrictions are feasible but only under certain conditions such as (Mara and Cairncross, 1989):
USING CROP RESTRICTIONS IN LARGE AREAS
The Metropolitan Region surrounding Santiago, Chile, is an example of applying crop restrictions to an area where the river water supply is heavily contaminated due to unrestricted discharges and diversion of this water for irrigation spreads the wastewater over a large unrestricted area. Due to the high incidence of gastrointestinal diseases in the Santiago area and the 1992 outbreak of cholera, the Health Ministry started a crop restriction programme in the MR which excluded production of certain high-risk vegetable crops in highly contaminated areas. This was complemented by a public education programme designed to raise the public awareness of food hygiene and by an Agricultural Ministry programme to limit the movement of high-risk crops out of the Region, The high-risk crops were defined in Health Ministry Resolutions No 350/1983, 7,717/1991 and 10,111/1991 and generally cover vegetables grown close to the ground and those normally eaten raw. Because market demand
remains high, vegetable production has relocated within the MR and there has also been increased movement of vegetables into the MR from other areas of the country. Due to the unrestricted river discharges, there is no assurance that the new vegetable production areas are not contaminated also.
This effort described in the example in Chile has already produced a significant reduction in disease rates within the Santiago area for hepatitis, typhoid and other gastrointestinal diseases (Figure 3). However, the total number of cases remains high by international standards. One reason the disease occurrence remains high is the unrestricted irrigation of similar crops in other areas since government resources to implement a crop restriction programme over a large area is limited and the market demand for such crops remains very high.
Where any of the above circumstances do not prevail, a crop restriction programme will be difficult to enforce. Many feel that crop restrictions are not administratively feasible and that a strong institutional framework must be present with the capacity to monitor compliance (Shuval et al., 1986a; Blumenthal et al., 1989; Strauss, 1991). As a result, crop restrictions are only used with success in wastewater use projects such as Mexico City and Ica, Peru where use occurs in a defined area and there is strong central control over water distribution. Crop restrictions have been used with mixed success in areas where unrestricted discharges occur to a river. This leads to heavily contaminated water being subsequently diverted downstream for unrestricted irrigation. In these instances, the wastewater is not used in a defined area and the distribution of contaminated water is not centrally controlled.
Continuing high disease incidence in developing countries will keep the focus on crop production with contaminated water. International pressure is also being exerted as countries importing food from developing countries are requiring more restrictive health protection and product hygiene standards. Because of the increased emphasis on food safety, an effective approach to complement or substitute for a crop restriction programme would be to develop a programme that assures buyers that they were purchasing a high quality product or a product that was produced in a safe environment. Because the market demand for vegetable products will increase, efforts to educate and focus the consumer on the need to use a safe product should be done at the same time a programme is undertaken to certify the product was produced in a safe environment. This uses market pressures to force producers to use sanitary conditions when growing vegetable or other high-risk crops.
Policy considerations for certification
Vegetable production with contaminated water has been identified as one of the mechanisms of disease spread, including the spread of cholera in Latin America in 1991-92. In developing a successful programme to ensure consumer protection, both the crop and the water supply must be considered.
The strong and successful cholera eradication programme in Chile in 1992 resulted from a heightened awareness of product quality by the consumer. This resulted, in part, from a strong public education programme on disease control and an informal programme by farmers where they placed labels on their produce which declared their vegetables were produced with safe water. This served to focus the consumer on the quality of the produce and the quality of the water used. The emphasis of the consumer and the producer in this case was on prevention. It is recommended that any certification programme also promote prevention as its primary focus.
There are two approaches to certification that focus on prevention:
(1) certifying the quality of produce, or
(2) certifying that the product was produced in a safe environment.
Controlling the quality of the produce (option 1), focuses prevention on the crop rather than on where and how it was produced as is done in option 2. Under option 1, assessment and control is done after the crop is produced and harvested. This is a high-risk programme not only for all producers but also for the group that is responsible for making the product inspections.
Conducting microbial testing of the crop is a highly specialized programme that requires sophisticated laboratory facilities and highly trained staff. Both of these are lacking in most developing countries. Producing marginal results or inaccurate results could lead to the programme having little or no credibility. The cost of operating and maintaining a continuous testing programme for the annual peaks in vegetable production will also mean maintaining a large cadre of trained staff and laboratory space to accommodate these peak times making the cost per sample tested very high.
There is considerable risk also to the agency responsible for conducting the testing. The product testing system must be highly efficient as most microbial tests require 24-72 hours to complete. This analytical time must be added to any sampling and data processing time, thus any delays would put the produce in jeopardy of spoilage. Most of the testing would be conducted on perishable fruits and vegetables where even a 24-hour delay may mean the loss of the crop.
The producers are also at considerable risk for their investment. If crop testing shows contamination, the crop may have to be destroyed as there likely would not be sufficient time to retest the crop before its quality and durability are lost. Any delays in the testing programme would put the crop at considerable risk. Most vegetable farmers in developing countries are small farmers where a crop loss might mean the total loss of income for that year or loss of a farming business.
Because of the high risk, the high cost, and the lack of well defined methods that ensure all crops are contamination free (option 1), this type of programme is not recommended. The most effective programme, at the present time, is to develop a certification programme that emphasizes prevention of contamination during crop production by developing safe production areas and ensuring the consumer that the product came from a clean area (option 2). A programme that focuses on safe production methods significantly lessens the risk to any user of the produce as well as to the producer. This type of focus has already resulted in a significant reduction of gastrointestinal diseases in the Santiago area of Chile as a result of an informal programme already started there by producers (Figure 3).
A certification programme should promote production of vegetables needed by the country but with the recognition that disease outbreaks have been caused by unrestricted irrigation with contaminated water. Experience shows the primary source of contamination in recent disease outbreaks was the water used for irrigation. Because of this, a certification programme should focus on the source of water used in production. The intent is to promote safe production with the ability to abandon the programme when the source of water contamination is controlled. It is recommended that the programme be conducted in close cooperation with National and Regional Health Services.
If the focus is on the water supply, a four-step process is suggested to ensure sufficient quantities of safe vegetables are available for the domestic market. The four-step process is:
(1) certify the water used in production (this step significantly reduces contamination risk);
(2) establish areas of clean water for vegetable freshing (this step reduces the threat of secondary contamination from use of poor quality water during harvest and transport);
(3) certify the water used in packing and processing the vegetables (this step reduces secondary contamination in packing and handling); and
(4) test a selected number of crops and certify the absence of contamination (this step assures the consumer that the product has been tested and found to be low risk).
An effective certification programme can begin with just the first step. Each new step would be taken only if:
In order for the agricultural and health authorities to control cropping and carry out an effective nationwide or regional water quality certification programme, there needs to be a sound information base for decision making (see section Evaluation of irrigation water quality).
The goal of a water quality monitoring and certification programme is to identify large irrigated areas that can be safely used to meet national vegetable production needs.
To carry out this programme, a staff needs to be developed who can plan, execute and interpret a water quality monitoring programme using the guidelines previously discussed in this document. This professional team should be utilized to establish the sampling and analytical procedures, define the monitoring points and conduct the initial area-wide monitoring assessment.
Having centralized control of the monitoring programme would help to:
The water quality monitoring programme should aim to:
Developing a water quality certification programme
The actual institutional structure should be developed when the programme is implemented. In choosing an approach, the primary goal must be to maintain a high degree of credibility with both the consumers and the producers as this is an attempt to raise their awareness of the need to minimize the risk to the consumer. The following principles should guide the development of a programme:
There are a number of ways to institutionalize the certification of water quality for promoting safe production areas. The choice of a structure depends greatly on the staff and financial resources that are available. Presented here will be two distinctly different approaches that depend upon how intensively water quality is assessed but both attempt to have the water quality samples, as closely as economically practical, reflect the quality used on individual farms. The two approaches discussed here are:
FIGURE 11: Certification of irrigation water quality used in a large irrigated area for vegetable crop production
Certifying a large area
One approach to water quality certification is to monitor quality at or near the main water supply point and assume that this quality represents the water that is distributed and used throughout the irrigation network. Certifying the quality of water used in a large irrigated area treats the entire area as one unit or field. This approach is similar to the basis used for the 1989 WHO Guidelines on wastewater treatment for irrigation use. The WHO Guidelines are goals for the discharge from a wastewater treatment process. As such, applying them as performance goals for the effluent assumes that the quality does not degrade after the effluent is discharged from the treatment plant and distributed and used throughout the irrigation network. The approach outlined here focuses on assessing quality at key points that represent both primary and secondary levels of contamination. This follows closely the monitoring procedure outlined in the section Criteria used to select field
monitoring sites. Figure 11 shows a five-step process for certifying clean production areas that focuses on the quality of water used in a large irrigated area. This approach will minimize the expenditures by the Agricultural Ministry. The most important factor in the success of this approach is the correct choice of monitoring sites.
The first step of an area-wide certification process is to assess the quality of the source water being diverted into the irrigation system (Figure 11). This step measures the level of primary contamination or contamination that occurred before the water was diverted for irrigation. To conduct an initial monitoring for primary contamination, the steps taken include those described in the section Evaluation of irrigation water quality. The financial burden for the first step needs to be assumed by a public agency as the goal is to define broad areas that can be considered as potentially safe production areas in order to meet national production needs for vegetable and other high-risk crops.
The second step in Figure 11 should also be carried out by a centralized authority. The goal of the second step is to measure the extent of secondary contamination occurring in irrigated zones that have a high potential for safe production (primary contamination levels <10,000 faecal coliforms per 100 ml). The first two steps closely follow the guidelines discussed in section Criteria used to select field monitoring sites. The first two steps will also help establish a legal basis for decisions, thus reducing the number of future samples that need to be taken by public agencies or individual producers.
After completing the water quality assessment phase (Step 1 and 2), the focus shifts to whether the entire irrigated area can be certified as a safe production zone. This shift assumes that the water used by an individual field or grower is similar in quality to that which was assessed at several widespread points within the irrigated area. The risk is that secondary contamination may occur below the selected monitoring point; however, as discussed earlier, if monitoring points are carefully chosen, the worst contaminated areas will be identified and excluded from further consideration. The monitoring points are also chosen so that if further contamination occurs below these points, it would be insignificant on a national or regional scale. The end result is that the potential for disease spread is significantly reduced.
To focus attention on safe production areas, the health or agricultural authorities can promote these areas by issuing a provisional certificate that can be used on produce from that irrigated zone (Step 3, Figure 11). This certification would be voluntarily used by producers with periodic compliance checks from the health or agricultural authorities. The certification programme should be developed and promoted in such a way that the producers have economic advantages by using the label.
FIGURE 12: Routine analysis for faecal coliforms (FC) in irrigation water used in large irrigated areas certified as safe production zones
Any programme, regardless of how rigidly structured, has the potential for fraud. The greatest threat of fraud with area-wide certification is that the producers or packers can label products from outside the certified area. There is no way to avoid fraud but, when found, the health and agricultural authorities should take swift and firm action. The most effective action is to remove the area certification. This unfortunately punishes all producers in that area for the actions of a few, but it also uses strong peer pressure because all the producers must live together and share resources.
Monitoring and certifying the quality of water used in a large irrigated area must be done by a public agency. This places the initial financial burden on the public agencies but is needed in order to maintain strict quality control over the initial assessments (Step 1). Periodic follow-up checks need to be made (Step 4) and this financial burden could be carried by the beneficiaries. It is recommended that resampling be done every two years at the monitoring sites established by the public agencies in Steps 1 and 2 of Figure 11. A flow diagram for taking follow-up samples is given in Figure 12.
To reduce the cost of follow-up sample analysis, the simplified MPN analytical procedure suggested by Mara and Cairncross (1989) should be used in those areas that have previously shown low faecal coliform levels. A description of the simplified procedure appears in the shaded box. A more detailed appraisal of actual levels of contamination needs to be conducted in all areas seeking first time certification or areas that have been previously withdrawn from the certification programme.
The concept of promoting safe production areas is further emphasized in Step 5. Areas where heavy initial or secondary contamination has been found have been excluded from the certification programme. The National or Regional Health Services are also likely to enforce crop restrictions in these heavily contaminated areas. Close cooperation with the health authorities will facilitate this effort.
Where only secondary contamination less than 10X the WHO Guideline has been identified as the reason certification is being denied, close communication should be established with the canal associations, rural community leaders and producers. Secondary contamination is a local problem and results from discharges directly to the canal system. By being excluded from the economic advantages that are likely to occur from the sale of high quality and safe produce, these groups could be influential in achieving clean up of the secondary contamination problems. The discharges are localized and the best solutions often come from having local groups solve local problems. If the local contamination problems can be solved, this area should be permitted to retry to enter the certification programme. As shown in Step 5, the cost for any new water quality samples, however, should be borne by the producers.
One difficulty with the periodic checks described in Figure 12 is that the quality of canal water-supply may vary. Areas that were initially considered safe may slightly exceed the WHO Guideline value of <1000 FC/100 ml. The dilemma is whether these areas should now be excluded from the water quality certification programme. In areas that were previously certified and routine follow-up monitoring now shows exceedance that is less than 2X the WHO Guideline value, these areas likely present a limited risk for disease spread. This is based upon the knowledge that the water quality monitoring points were established to assess area-wide quality and that 1-3 days of additional travel time may still be needed for the water to reach the field. Kittrell and Kurfari (1963) show that additional reductions in coliform bacteria are likely to occur with time as the water flows in either a river or canal.
SIMPLIFIED ANALYSIS FOR FAECAL COLIFORMS
This procedure tests whether or not wastewater meets the 1989 WHO Guideline of 1000 faecal coliforms per 100 ml for unrestricted irrigation.
Use normal aseptic procedures throughout. Prepare a 1 in 10 dilution by adding 1 ml of diluted sample to each of 5 tubes containing 5 ml of A-1 medium and a Durham tube. Incubate at 44.5°C for 19-23 h, Count the number of positive tubes (those showing gas production), and read the most probable number (MPN) of faecal coliforms per 100 ml of wastewater from the following table.
Number of positive tubes
MPN of faecal coliforms per 100 ml
Source: Mara and Cairncross (1989)
The slightly elevated levels that show exceedances less than 2X the WHO Guideline value should remain a concern and should not be tolerated as a long-term water quality condition. Figure 12 suggests a 1-year provisional certification can be granted to areas with a faecal coliform value between 1000 and 2000 FC/100 ml. If after 1 or 2 years the sources of contamination have not been identified and controlled to < 1000 FC/100 ml, the provisional certification should be removed. Such an approach would meet the goals of the certification programme which are to promote safe production areas and use economic incentives or disincentives to promote clean up of slightly contaminated areas.
Certifying individual producers and fields
This alternative approach is to certify the water used on a specific crop or field. Water quality samples must then, as closely as economically practical, be taken to reflect the quality used on each individual farm throughout the irrigation season. Monitoring at each field attempts to improve on the disease reduction rates that can be accomplished with the area-wide approach.
The steps in the process are outlined in Figure 13. The first step is to establish the water quality conditions for the area. This includes defining primary and secondary contamination. The level of water quality sampling that occurs in Step 1 is identical to that used in certifying large irrigated areas (Figure 11). As in the previous approach, the focus is on defining safe production areas. The financial burden for this initial assessment needs to be assumed by the public sector in order to maintain strict quality control over the data.
FIGURE 13: Certification of irrigation water quality used on individual fields for vegetable crop production
The goal of this initial step is to establish a legal basis for decisions and reduce the number of future samples that need to be taken by individual producers. By defining in Step 1 those areas that have the greatest potential to be safe production areas, the agricultural authorities can limit the areas that should be considered for certification. This avoids the agricultural authorities having to reject a number of producers who had little chance of certification because of the extent of contamination. The agricultural authorities can then focus their resources on safe production areas.
The method used to evaluate the existence of primary contamination is identical to that used in the previous approach. The initial evaluation criteria should be <10 000 FC/100 ml (10^4) rather than the WHO Guideline of <1000 FC/100 ml (10^3). As discussed in the previous section, it is not the intent at this level of monitoring to certify water that is less than 10^4 FC/100 ml as safe for use in vegetable production. The goal is to define on a regional basis those areas that have the greatest potential to be safe production areas.
Defining primary contamination, however, is only one part of the first step. A similar assessment must be conducted for the extent of secondary contamination (section Criteria used to select field monitoring sites).
In the alternative that focuses on a specific crop or field, all areas that show less that 10^4 FC/100 ml would be considered as potential safe production areas. Then it is the individual producer who must petition the Agricultural Ministry to be included in the programme (Step 2 in Figure 13). The Ministry must then decide whether to require the individual producer to conduct additional water quality testing to ensure that clean production standards are used or whether the initial monitoring assessment conducted in Step 1 is sufficient. Any additional monitoring then becomes the responsibility of the individual producer (Step 3).
There is an exception (as noted in Step 2) to the exclusion from consideration when the initial assessment shows heavy contamination (>10^4 FC/100 ml). Where groundwater is being used as the only source of irrigation water within a known heavily contaminated area, this groundwater area could be considered for inclusion in the certification programme. Groundwater, even in heavily contaminated areas, rarely shows signs of bacteriological contamination. This source of water offers a high potential for safe production; therefore, these areas should be considered for inclusion in the certification programme even if located in a heavily contaminated area (> 10^4). The use of groundwater, however, should not be the only criterion used for certification. The producer must be able to demonstrate that the canal water source has been eliminated and that no secondary sources of contamination affect the groundwater supply canals. This should be a prerequisite to water quality
certification for groundwater use areas.
In the area-wide certification alternative described in the previous section, the health and agricultural authorities collected, analysed and assessed water quality samples and issued a certification to the irrigated area. In contrast, in this alternative the focus is on an individual crop or field, and the health and agricultural authorities must not only be prepared to conduct an area-wide water quality assessment but must be prepared to evaluate the water quality analyses submitted by the individual producers. Where the irrigation supply meets the WHO analyses submitted by the individual producers. Where the irrigation supply meets the WHO Guidelines of <10^3 FC/100 ml, the Agricultural Ministry must be prepared to authorize the producer to use a certified label. The label would state that the agricultural and or health authorities have certified that the water used in production meets health guidelines (Step 4). This label can then be applied to produce from that field
if the producer has met the following series of administrative conditions (Step 5):
Certifying individual fields gives the health and agricultural authorities close control of labelling and distribution of produce. The Agricultural Ministry in return must be prepared to make quick administrative decisions to ensure the availability and timely distribution of produce. Closer control means continuous financial resource needs for the Agricultural Ministry especially if they certify areas in remote regions.
The key element in an effective programme is strong water quality monitoring. All the water quality sampling in Step 1 of Figure 13 should be used until experience is gained about the water quality within an irrigated area. The goal should be to repeat a similar area-wide assessment at least once every two to three years. Where monitoring is conducted at the field level by individual producers (Step 3), this should also be repeated every two years. After experience is gained, the monitoring in Step 3 can be reduced. A description of the process for repeating the analysis by individual producers (Step 6) is given in Figure 14. The procedure in Figure 14 is similar in concept to that described for the area-wide assessment.
Repeated sampling for certification of water used on a specific crop or field will be faced with the dilemma of how to handle areas that have previously shown compliance with the WHO bacterial guideline but now slightly exceed the WHO Guideline value (<2X the guideline value). Water quality samples are being collected near the field after the water has travelled many kilometres through earthen lined canals in rural areas. The sources of contamination under such circumstances can be many, including non-human sources (animals, wildlife, etc.). While this water presents a disease transmission risk, that risk is significantly below that seen closer to urban areas where the level of contamination often exceeds 10^4 FC/100 ml. For the areas that were initially below the WHO bacterial guideline but now have faecal coliform levels from 1000 to 2000 FC/100 ml, the emphasis should not be on exclusion but should be directed at trying to identify the sources of contamination to make
this a safe production area.
A provisional certification can be used for areas that only slightly exceed the WHO Guideline. Provisional certification is not to promote these areas as safe production areas but to promote these areas as having significant potential to become safe production areas. Use of a 1-2 year provisional certification assumes a risk but that risk is far less than the unrestricted use of contaminated water for vegetable crop production in other areas. Strict adherence to the limited time period will diminish the future need to utilize the provisional certification. As sufficient safe production areas are identified, use of the provisional certification could eventually be abandoned as a step in the certification process.
FIGURE 14: Routine analysis for faecal coliforms (FC) in the irrigation water used at the field level in areas certified as safe production zones
With increased consumer awareness of the need for hygienically clean produce, there may be an informal labelling of produce. This would be an effort by growers to ensure the consumers of their product safety. There is a high potential for fraud with any informal system. The Agricultural Ministry needs to standardize the use of certification labels to ensure consumer protection and place a high degree of credibility in the labels being used.
The proposal described in Figure 13 also has the potential for fraud. The potential for fraud, however, should not diminish the desire of the agricultural and health authorities to use a standard certification label. The economic advantages of using the label should be an incentive by the users to avoid fraud. Likely sources of fraud are:
There is no way to avoid fraud but, when found, the agricultural and health authorities should take swift and firm action. This will strengthen the role of the Agricultural Ministry in its efforts to promote safe production areas and reduce the possibility of fraud in the future.
The two alternative approaches presented in this and the previous section emphasize promoting safe production areas but each requires a different level of resources and administrative backing. Both proposals should only be considered as concepts. The actual development of a certification programme must consider several other factors. A detailed discussion of such factors can be found in a report on a water quality certification project in Chile (FAO, 1993). There are three factors that this report concluded were important to the success of any programme:
Developing and operating a certification programme must have the necessary resources to be done with high standards and a high degree of credibility. In developing countries it may be necessary to structure and implement a pilot programme in water quality certification for safe vegetable production. This pilot programme would give the initial programme high visibility. Seeking international assistance in structuring and implementing a pilot programme or in conducting the nationwide water quality assessment would also give the programme the needed credibility with the affected farmers as well as the consumers, both national and international.
Role of public service
A primary role of public service in food supply is consumer protection. This includes establishing national policies and programmes that promote and provide a safe and readily available food supply. The use of contaminated water to irrigate vegetables and certain fruits in Santiago, Chile, was identified as one of the chief means of spreading certain diseases including cholera (Shuval, 1993). Therefore efforts in the public sector must be strengthened to develop a safe food supply that meets national needs. Some of the considerations that are needed by the public service are:
A programme to certify the bacterial quality of water used in production of high-risk crops such as vegetables focuses on controlling a public health problem. As such, the Health Ministry has the primary role for public health. The health authorities should set the boundaries and limitations for acceptability such as the WHO Guidelines (WHO, 1989) and the role of Agricultural Ministry should only be to implement programmes which promote safe agricultural production within the limitations established by the health specialists.
The health authorities carry the primary role in setting the health standards for water utilized for irrigation. The present standard as described in the WHO Guidelines should remain the goal of Agricultural Ministry. The Agricultural Ministry should evaluate, however, whether national production goals can be met while meeting this standard. If clear evidence is available that national production goals cannot be met or that negative economic conditions will result, the Agricultural Ministry needs to consult with the health authorities regarding a temporary modification to the regulation. It is strongly felt that the regulation may be very restrictive for existing conditions in many developing countries and that the Health and Agricultural Ministries will need to evaluate closely the data collected during a water quality monitoring programme (see discussion in the section Alternate irrigation methods later in this chapter).
A coordinated programme among the agricultural, irrigation and health authorities can make a crop or water quality certification programme much more effective. The focus of the agricultural and irrigation authorities must remain on production and supply issues while the health specialists need to focus on consumer protection. The agricultural authorities should emphasize certification of safe production areas or safe production techniques to enhance the ability of the producers to respond to health restrictions. This emphasis would also enhance the ability of the health authorities to restrict cropping in non-certified areas or control the distribution of non-certified produce. Controlling or restricting cropping for health reasons should remain with the health authorities as the restriction is being done for a public health reason.
The health specialists should take the lead in developing and maintaining a bacteriological laboratory quality control programme for private laboratories since this is outside the expertise of agricultural specialists. The choice, timing and frequency of water quality sampling, however, should remain with the agricultural authorities since they have the expertise to define representative sites within an irrigated zone.
The agricultural and health authorities could jointly strengthen the role of the certification programme by using public education techniques to emphasize to consumers and vendors the need to buy only certified produce. In addition, the use of fraudulent labels could be reduced if the health authorities strengthen their regulations against false advertisement or health protection claims on labels.
An important new role for the national and regional agricultural and irrigation authorities is ensuring that the needs of production agriculture are thoroughly considered in plans to treat and dispose of urban wastewater. It is only under a coordinated plan that the agricultural sector can proceed to make long-term safe production areas without such a programme becoming a financial drain on public resources.
There is another level of the public sector that is important to the success of a national water quality certification programme: the local service agencies that serve the irrigation water user. Their role is to ensure a steady flow of the resources (financial, water, seed, etc.) needed to produce a food supply. These local agencies, especially those that supply water, must focus on ensuring that the resource they supply does not affect the safety of the food being produced.
An important local group is the canal or irrigation association. They control how water is diverted, distributed and often what other water enters the irrigation supply canals. Because of the importance of contamination that occurs in the irrigation canal network, these associations hold a major key to ensuring a safe water supply. Often those discharging into the irrigation system are also water users or are directly connected with the agricultural system in the area. Each holds a stake in ensuring a safe water supply but often the results of their joint or individual actions are not well understood. These local agencies could act as a focal point for national health and agricultural authorities which are attempting to clean up or promote safe production areas. The local agencies are an important factor in the area-wide certification alternative described above.
Role of the private sector (producer, packer and shipper) in certification
The private sector growers hold the key to the success of a water quality certification programme. The programme to label safe produce must be voluntary to be effective. The public service agencies have limited resources for regulation. Structuring the programme must be done with the view of minimizing the burden on the public service agencies and the producers and stress that compliance with the programme will bring the producers economic advantages. Informal programmes are likely to occur in the future and these will be driven by economic incentives. In any formal programme, the private sector must be willing to self-regulate itself with the punishment for failure being a loss of economic advantage.
To promote self-regulation, the programme must have several checks and balances built in but rely heavily upon economic incentives. This process must be driven by the consumer. The alternative and more difficult approach is for the health authorities to extend the present restrictions on production of high-risk crops in contaminated areas to include restrictions on the sale of non-certified produce. This approach is very resource-intensive for the public service.
The key to success is to ensure that the producers included in the certification programme have an economic advantage. Public authorities will incur expenses in administration of the programme. The logical step is to impose a fee on the producers that covers part or all of the expenses of administrating the programme. With refinements in the programme and as experience is gained, the agricultural authorities could consider in the future turning the entire programme over to the producers with only minimal checks from health and agricultural authorities.
The private sector and especially the producers need to become more involved in seeking solutions to the contamination problems that are restricting their production. As success is achieved in solving these problems, the costs to the producers and the restrictions on production will diminish and eventually the need for a certification programme will disappear. The alternative is to live with the restrictions but the opportunity to be proactive in setting production targets and gaining valuable consumer support will be lost.
Institutional and policy issues
Water certification
Policy issues for development of safe production areas
Water certification
Worldwide many countries are beginning strong cholera eradication programmes which have resulted in a dramatic decline in the number of gastrointestinal diseases, including cholera. The dilemma for these countries is that cholera exists as an endemic disease in neighbouring countries and can be quickly reintroduced if a high vigilance is not maintained. The focus of many of these programmes has been on crop restrictions especially where vegetables are produced with water heavily contaminated with human wastewater. This type of production has been identified as one of the primary mechanisms of disease spread.
Crop restrictions require continued vigilance, an expense most developing countries can not afford. Success also requires that there be little or no market pressures for the restricted production. Therefore, it is necessary to develop adequate production areas. To support an effective crop restriction programme, the Agricultural Ministry must focus on developing safe agricultural production areas. This focus must be supported by a programme to heighten the awareness of consumers to the need for high product quality. One method to develop safe areas is to initiate a certification programme for the source of water used in production.
It is recommended that a strong information base be developed by initiating this assessment programme nation-wide. Assessing water quality can go forward in conjunction with a programme to issue a certified label for the safety of the water or prior to such a programme. In order to operate a nationwide assessment programme, it is recommended to develop, within the natural resource group of a public agency, a staff who can plan, execute, and interpret a water quality monitoring programme using the guidelines developed in this document. This staff will require expertise in hydrology or sanitary engineering supported by expertise in irrigation and agronomic sciences.
The cost of the assessment programme must be borne initially by the public agencies. The options for alternative financing are:
The Agricultural Ministry should not become involved in restricting production in contaminated areas. Such a restriction is for public health reasons and health matters are the expertise of the health authorities. A certification programme, therefore, needs to focus on promoting safe production. Where testing meets the health standards, there needs to be a formal programme to recognize the safety of this production. At present, there are informal programmes of labelling being initiated by the producers to assure consumers of the safety of their product. There is high potential for fraud under an informal programme and there needs to be a standardized use of labels certified by a public agency to ensure consumer protection.
Using a public agency to initialize such a programme provides for uniformity of application and the capability to provide widespread application. The long-term policy of a country must be to place control of product quality, including certifying clean production areas, in the hands of the producers and packers. This approach, however, cannot be recommended at this time because of the need to maintain a high vigilance to keep the agricultural sector from being the cause of the spread of cholera or other diseases and to establish a high visibility and credibility with both consumers and producers.
The best approach is for the Agricultural Ministry to initiate the use of a label that certifies that the water used in production has been tested and meets the Health Ministry and WHO standards for safety. For such a programme to be effective, it must be:
The labelling of produce should be controlled by the agricultural authorities through direct contact with each producer or packer regardless of whether the water testing was done on an individual field basis or on an area-wide basis. Until there is a better understanding of water quality conditions in an area, the individual producers will incur expenses to test their water supply.
Permitting use of a certified label from the Agricultural Ministry should be based upon the water testing conducted by either the individual producers or the Agricultural Ministry. In order to make the programme operate effectively, the Agricultural Ministry must:
The programme outlined above is based on a four-step process (see Policy considerations for certification).
Policy issues for development of safe production areas
Long-term policy on wastewater use
The long-term policy for urban wastewater use in most developed and in many developing countries, regardless of the level of treatment, is to restrict the irrigation use to a well defined area. Examples of this policy are the Werribee Farm in Melbourne Australia, City of Modesto Farm in Modesto, USA, Kuwait and Number 03 Mezquital Irrigation District of Mexico (Pescod and Arar, 1988). Cropping in these defined areas is tightly controlled, thus minimizing the resource needs of the Health and Agricultural Ministries to ensure safe crop production and the absence of disease spread. The Agricultural and Health Ministries should promote a national movement toward this policy as wastewater treatment facilities are established in urban areas.
Such restricted areas would eliminate the need for widespread monitoring and crop restrictions as well as promote higher health standards in the rural areas. With increasing demands for higher health standards, unrestricted use of wastewater will continue to produce a negative image of all agricultural practices and products within countries still permitting unrestricted discharges. In the interim period, however, the Agricultural Ministry, in cooperation with the health authorities, must try to control cropping at the point of water use. This will continue to be a resource burden for the country. The programme suggested in this document should assist with policy options for this interim period but should not be considered the long-term policy.
In many developing countries, adequate wastewater treatment is many years away and unrestricted wastewater discharges to rivers, streams and canals will continue for the foreseeable future. Regardless of whether treatment can be provided in the near future, the agricultural and health authorities should promote the policy that these flows should be collected and routed to a defined use area, thus preventing their unrestricted discharge to surface waters or irrigation canals. Cropping practices within this wastewater use area can then be tightly controlled thus avoiding the expense of implementing widespread crop restrictions. Compensation may need to be given to the farmers in the use area, however, having one heavily restricted area is a far better resource management approach than paying the health and lost financial resource costs of continued unrestricted discharges that affect widespread areas.
Coordinated planning for contamination control
Wastewater treatment at the source of contamination is the most effective means of establishing clean vegetable production areas. This approach is used in the United States, Canada and Europe. In addition this policy is supported by the WHO Guidelines (WHO, 1989) and the World Bank (Shuval et al., 1986a). In most developing countries, the agencies responsible for treatment do not fully consider the development of safe production areas in establishing their priorities for treatment facility construction. The reason is that many regional wastewater treatment plans were developed prior to the increased consumer awareness of product safety. In addition, the rapid spread of cholera has renewed the need to establish these safe areas. This need was not considered when initial priorities were set. For example, a 1992 project showed that less than 2 percent of the present wastewater flow from the urban Santiago area of Chile is being discharged into the Maipo River. This discharge,
however, contaminates almost 50 percent of the high quality lands that are presently being affected by discharges from the urban areas (FAO, 1993).
The lack of coordinated planning for controlling wastewater discharges will continue to impede agricultural and health programmes to establish safe production areas. Because of the importance of establishing safe production areas, the Agricultural Ministry should promote the development of an agency or the strengthening of an existing central agency that sets discharge standards or sets priorities for wastewater treatment implementation. This agency should establish a timetable for meeting established goals and review contamination reduction efforts to ensure these programmes meet the goals established. Only with this type of coordinated planning can programmes of the agricultural and health authorities be expected to develop the agricultural resources now being impacted by such discharges.
Other sources of contamination
In a project in Chile (FAO, 1993) secondary sources of contamination (discharges directly to irrigation canals) were identified as a major constraint to future development and maintenance of clean production areas. For example, in a typical irrigated region of Chile, almost 65 percent of the region was affected by such discharges. This irrigated region is similar to many throughout Latin America or in other developing countries. The percentage of the area affected by these discharges is likely to increase significantly when individual producers begin sampling further within the irrigation system than has been sampled up to now. The sources, cause and methods to eliminate this contamination are not well understood.
FIGURE 15: Distribution of faecal conform levels found within the intermediate contamination level (1000-10 000 FC/100 ml) for the Metropolitan and V Regions of Chile, 1992-93
(Source: FAO, 1993)
Because of the importance of secondary contamination to the success of a crop or water quality certification programme, the Agricultural Ministry, in cooperation with the Health Ministry, should consider seeking international assistance in formulating, developing and implementing such a project. The goals of the project would be to evaluate the sources of secondary contamination, recommend methods to eliminate these discharges, implement these techniques in a pilot area and evaluate the success of the measures. The objective would be to develop a technique or procedure that could be transferred to other areas of the country to establish clean production areas.
Health standards and agriculture
It is recognized that the Health Ministry carries the primary role for setting health standards for water utilized for irrigation. Many countries have existing standards and these should be used by any programme to certify safe production areas. In the absence of a national standard, the present guideline defined by WHO (1989) should be the goal.
The certification of water as safe for production using the procedure outlined in this document attempts to adapt an engineering design guideline of WHO (1989) into a field water quality evaluation tool. Like a wastewater treatment process, field conditions vary with time. Immediate compliance with the guideline may not be practical especially where secondary contamination is present and the sources are not well defined. It may be necessary to establish an interim performance goal coupled with a plan and timetable to eventually meet the WHO Guideline. It is important to remember that a guideline or regulation is only as good as the efforts to meet it. If a regulation is unattainable, no one will attempt to meet it. On the other hand, if a logical plan with a timetable is developed, most will attempt to meet it if given the time to adapt. Therefore the best approach in many developing countries is to establish reasonable plateaus of progress, each moving toward a lower health
risk from utilizing wastewater.
If a full programme to restrict cropping in all areas that exceed the WHO Guideline is implemented, meeting production goals may be difficult. If there is clear evidence that national production needs cannot be met or that negative economic conditions could result, the Agricultural Ministry needs to consult with health officials about a temporary modification to the WHO Guideline in these areas.
No specific guidance is offered on which areas to focus on for a temporary modification, but from a disease risk basis it would appear best to focus on:
When a temporary modification to the WHO Guideline is used, that area should be considered as a non-certified area or provisionally-certified area. Both the agricultural and health authorities need to establish a time schedule to eventually achieve the WHO Guideline in the non-certified or provisionally-certified areas. This compliance period allows producers and others sufficient time to reduce secondary contamination or face the loss of future production opportunities. The goal is to exclude producers from production if they do not play an active role in helping meet national goals for safe production.
Alternative irrigation methods
The WHO Guideline for wastewater use is based upon avoiding direct contact between the product and the wastewater. The main focus has been on vegetable crops that are normally eaten raw. Little consideration has been given to fruit orchards, vineyards or other vegetables that do not grow close to the ground because all of these have a low potential to come into contact with the irrigation water.
This situation changes dramatically if the method of irrigation is changed. Sprinkler irrigation presents an increased concern because the water is sprayed directly on the vegetable crop or fruit thus increasing the risk of contamination. The Agricultural Ministry, in cooperation with health authorities, should discourage the introduction of sprinkler irrigation in any irrigated zone found to have supply water with FC in excess of the WHO Guidelines. Sprinkler irrigation presents an increased risk of disease transmission; regardless of whether the production is vegetables or fruits this will only produce a negative image of all produce from that region or country.
Role of the Health Ministry
Because of the public health concern, the Health Ministry carries the primary responsibility for restricting crops in areas that use water not meeting the WHO Guideline or national standards. The information developed by the water quality certification programme should strengthen their efforts to identify contaminated areas and fields.
The Health Ministry must also promote the Agricultural Ministry label as the only one that meets the Health Ministry requirements. Health authorities could reinforce the agricultural programme by adopting a regulation that states if a product carries a label stating the quality of the product, the producers must be able to prove their claim. This can only be done for vegetables with a certificate from the agricultural authorities stating that the production was done with water that meets standards established by the Health Ministry.
The health and agricultural authorities could strengthen the role of the certification programme by using a public education programme to emphasize to consumers and vendors the importance of buying only certified produce. In addition, the Health Ministry needs to cooperate closely with agricultural authorities to develop methods to control the causes of secondary contamination in the irrigation channels.
<section>7</section>
Chapter 5 - The case study of Chile
Planning for controlling the quality of irrigation water destined for vegetable production in Chile
Planning for controlling the quality of irrigation water destined for vegetable production in Chile
The outbreak of cholera in Chile in April of 1991 made it urgently necessary to control the use of contaminated irrigation water used on vegetable crops that are normally eaten raw. The Ministry of Health began a major programme to restrict such production in heavily contaminated areas. This increased the urgency for the Servicio Agrícola y Ganadero (SAG) of the Ministry of Agriculture to evaluate the extent of irrigation water contamination and to develop methods to promote production in safe areas.
FAO together with SAG developed a one-year international project (Oct 1992 - Sept 1993) to evaluate the impact of microbial contamination in irrigation water. The project counterpart was the Department of Natural Resource Protection (DEPROREN) of SAG. The principal objectives of the project were to evaluate the existing levels of contamination, develop a database that could be used as a basis to control contaminated water use in vegetable production and to propose a certification system that could be used to promote safe production areas for both internal and international markets.
The project was operated in three distinctly different phases:
Phase 1 (Water Quality Monitoring Phase)
In developing and conducting the water quality monitoring, the project reviewed whether the procedures had national and international recognition, formed a legal basis for follow-up actions and whether SAG could use the procedure and results to promote safe production. Monitoring was conducted in only two pilot areas; the Metropolitan and V Regions of Chile. These areas represented 50 percent of the population and 49 percent of the total vegetable production in the country. In addition, the two Regions contain 80 percent of the production of the 14 crops identified by the Ministry of Health as high-risk crops.
The project used faecal coliform count as the contamination indicator. This indicator is recognized internationally by WHO, PAHO and other organizations in Latin America. Faecal coliform count is also used in regulations in Chile that deal with wastewater and how it can be used on crops (NCh 1333/1978) and is within the capability of all public and private laboratories in Chile to determine on a routine basis. The project recommended the continued use of this indicator until changed by the Ministry of Health.
TABLE 14: Distribution of faecal contamination in irrigated areas of the Metropolitan and V Regions, Chile
Faecal contamination level (Faecal coliforms/100 ml)
% of the 120 000 ha sampled in the Metropolitan Region
% of the 80 000 ha sampled in the V Region
The analytical method used by the project was the multiple-tube fermentation technique. This method was recommended for future programmes within Chile but the project felt strongly that SAG, the Ministry of Health and the University of Chile should attempt to adapt the membrane filtration method to use with Chilean irrigation waters. This would give SAG and others much more flexibility in sampling in remote rural areas and responding to emergency situations throughout the other regions.
The project used the University and Public Health Institute (ISP) laboratories for analytical work. The main reason for using these laboratories was the lack of capabilities within SAG laboratories and the need to demonstrate a high level of credibility during the monitoring programme. The project recommended that SAG continue to strengthen its ties with the ISP laboratories until capabilities within SAG laboratories could be established.
Two contamination sources were considered in setting the water quality monitoring sites: primary contamination that occurred in the rivers before the water is diverted into the irrigation system and secondary contamination that occurred within the irrigation system. The project developed guidelines for selecting monitoring sites based on these two contamination sources and complying with existing regulations as defined in NCh 1333/1978. The guidelines emphasized only monitoring potentially clean areas to promote safe production. This approach avoided using financial resources to monitor heavily contaminated areas that have little potential for future production of vegetables.
The monitoring network set out by the project divided the Metropolitan Region into five irrigated zones. From December 1992 to March 1993, a total of 604 samples were collected from 120 sites within these five zones which cover approximately 120 000 hectares. The V Region was divided into seven irrigated zones. From January to April 1993, over 750 samples were collected from 150 sampling sites within the seven zones that cover approximately 80 000 hectares.
Phase 2 (Data Analysis Phase)
Based on the monitoring results, the project was able to make the estimates shown in Table 14.
In order to evaluate the project results, the project assessed primary contamination. This contamination was generally due to discharges of untreated urban wastewater in amounts that were so large that often there was not sufficient natural river water to dilute the discharge to bring the faecal coliform levels below the 10^3 FC/100 ml standard designated in NCh 1333/1978.
The project also assessed secondary contamination. Some of the causes of secondary contamination have been identified as discharges directly to the irrigation canals of domestic household wastewater and from animal confinement facilities. The choice of monitoring sites focused on detecting the presence of secondary contamination in channels that initially showed low levels of contamination at the river intakes. The selection of sites followed the guidelines shown in Figure 7.
As shown in Table 14, the Metropolitan Region had the highest level of bacterial contamination. The main cause was the direct discharge of untreated urban wastewater to natural waterways before the water was diverted for irrigation (primary contamination) but secondary contamination sources did play a role. This results in only 8 percent of the irrigated area with a high potential to participate directly in a programme of water certification for safe vegetable production. Because of the extent of the primary contamination from discharges to the river, it is unlikely that the area available for vegetable production could increase significantly until the treatment works are in place for the urban wastewater. Because of the high initial levels of contamination, secondary contamination in such channels did not appear to be a significant factor with the exception of zones irrigated with well water or in areas not affected by large urban discharges.
It can be seen in Figure 8 that 37 percent of the irrigated area of the Metropolitan Region initially was below the maximum defined in NCh 1333/1978 but this dropped to only 8 percent after secondary sources of contamination were examined. This decrease was followed by a rise in the percentage of area found in the intermediate range (10^3-10^4). The increase in percentage of the area contaminated was due to discharges directly to the canal system downstream of the intake.
Figure 8 also shows that the percentage of area affected in the heavily contaminated areas (>10^4) did not vary significantly. These areas are affected by heavily contaminated discharges that were external to the irrigation system (primary contamination) and these high levels did not drop significantly as the water passed through the irrigation channels.
In the V Region, almost 1/3 of the irrigated area tested had a high probability for direct participation in a certification programme for safe production areas. The most important cause of contamination in the V Region was secondary contamination. An additional 63 percent of the irrigated area tested could participate in a certification programme if the direct discharges to the canals were eliminated (Figure 16).
Secondary contamination discharges were found to prevent a significant amount of the total irrigated area from immediately participating in a water certification programme. The sampling points for this programme were chosen to represent a large irrigated area and, as such, samples were often taken at intermediate points in the irrigation system. The extent of irrigated area that moves from the low (<10^3) to the intermediate range (10^3-10^4) or higher is likely to increase as water samples are collected closer to the individual fields. Thus solving the secondary contamination problem should not be overlooked as it may be a major constraint in developing safe (clean) production areas.
FIGURE 16: Changes in the extent of faecal contamination in the irrigation water of the V Region of Chile as a result of discharges into the irrigation system as compared to the initial level of contamination in the source of water used in the irrigation system
(Source: FAO, 1993)
The project recommended that SAG work with the Ministries of Health and Public Works to seek international assistance in developing and implementing methods to reduce or eliminate secondary contamination from areas showing high potential for safe vegetable production (areas <5000 faecal coliforms/100 ml).
Phase 3 (Crop Certification Phase)
The water quality data from Phase 1 and 2 were used to develop the concepts of a crop certification programme. The strong cholera eradication programme already instituted resulted in a heightened awareness of product quality by the consumer. The result was a number of labels used by producers that emphasize to the consumer the safety of the water used in production.
In order to implement this programme, SAG needed to be prepared to develop and operate a nation-wide water quality monitoring programme to assess the extent of irrigation water contamination. To carry out this programme, it was recommended that SAG develop a staff who can plan, execute and interpret a water quality monitoring programme. The procedures used in this project were recommended for use as the guidelines for conducting such a programme.
The project developed a five-step process for certifying clean production areas. The first three steps focus on water certification. The final two steps shift the focus of SAG to developing and applying a SAG label to vegetable production originating in safe production areas. The procedure for controlling the labelling of safe vegetable production was reviewed. It was recommended that, if resources are available, the certification of the water and the application of a SAG label should be controlled by direct contact with each producer. Control at this level ensures that certified water and safe production practices are used. Alternative and less intensive approaches considered were issuing labels on an area-wide basis or allowing groups of producers to control the labelling in SAG certified areas.
The project also recommended that SAG and the Ministry of Health strengthen the role of the certification programme by using public education techniques to emphasize to consumers the need to buy only SAG certified produce.
The scheme laid out by the project was considered a concept. The actual certification programme must consider several factors. There are four factors the project felt were extremely important to the success of any programme:
<section>8</section>
References
APHA. 1992. Standard Methods for Examination of Water and Wastewater. 18th Edition. American Public Health Association. Washington DC.
Arthur, J.P. 1983. Notes on the design and operation of waste stabilization ponds in warm climates of developing countries. World Bank Technical Paper No. 6. World Bank, Washington DC.
Bartone, C.R. 1991. International perspective on water resources management and wastewater reuse: appropriate technologies. Water Science and Technology 23(10/12): 2039-2047.
Blum, D. and Feachem R.G. 1985. Health Aspects of Nightsoil and Sludge Use in Agriculture and Aquaculture. Part III: An Epidemiological Perspective (Report No. 05/85). International Reference Center for Waste Disposal. Dubendorf.
Blumenthal, U.J., Strauss, M., Mara, D.D. and Cairncross S. 1989. Generalized model of the effect of different control measures in reducing health risks from waste reuse. Water Science and Technology 21(6/7): 567-577.
Bordner, R. and Winter, J. (eds.). 1978. Microbiological methods for monitoring the environment: water and wastes. US Environmental Protection Agency Report No. EPA-600/8-78-017. 338 p.
Cifuentes, E., Blumenthal, U., Ruiz-Palacios, G. and Bennett, S. 1991/92. Health impact evaluation of wastewater use in Mexico. Public Health Review 19:243-250
Duron, N.S. 1985. Mexican experience in using sewage effluent for large scale irrigation. In: Treatment and Use of Sewage Effluent for Irrigation (1988). M.B. Pescod and A. Arar (eds). Butterworth, London.
FAO. 1985. Water quality for agriculture. R.S. Ayers and D.W. Westcot. FAO Irrigation and Drainage Paper 29, Rev. 1. FAO, Rome. 174 p.
FAO. 1992. Wastewater treatment and use in agriculture. M.B. Pescod. FAO Irrigation and Drainage Paper 47, FAO, Rome. 125 p.
FAO. 1993. Control de Aguas de Riego Destinadas a la Producción Hortofrutícola: Chile. Technical Report of Project TCP/CHI/2251(A). FAO, Rome. 69 p.
Feachem, R.G., Bradley, D.J., Garelick, H. and Mara, D.D. 1983. Sanitation and Disease: Health Aspects of Excreta and Wastewater Management. John Wiley, Chicester.
Geldreich, E.E. 1976. Fecal coliform and fecal streptococcus density relationships in waste discharges and receiving waters. CRC Critical Reviews in Environmental Control. 349 p.
Gerba, C.P., Wallis, C. and Melnick, J.L. 1975. Fate of wastewater bacteria and viruses in the soil. Journal of the Irrigation and Drainage Division, American Society of Civil Engineers 28:987-991.
Hespanhol, I. and Prost, A.M.E. 1994. WHO guidelines and national standards for reuse and water quality. Water Research 28(1): 119-124.
IRCWD. 1985. Health aspects of wastewater and excreta use in agriculture and aquaculture: The Engleberg Report. International Reference Center For Waste Disposal (IRCWD) News 23: 11-18.
Kittrell, F.W. and Kurfari, S.A. 1963. Observations of coliform bacteria in streams. Water Pollution Control Federation 35(11): 1379.
Mara, D. and Cairncross, S. 1989. Guidelines for the Safe Use of Wastewater and Excreta in Agriculture and Aquaculture: Measures for Public Health Protection. World Health Organization, Geneva. 187 p.
Pescod, M.B. and Arar, A. (eds). 1988. Treatment and use of sewage effluent for irrigation. Proceedings of the FAO Regional Seminar on the Treatment and Use of Sewage Effluent for Irrigation. Nicosia, Cyprus 7-9 October 1985. Butterworths, London. 380 p.
Rose, J.B. 1986. Microbial aspects of wastewater reuse for irrigation. CRC Critical Reviews in Environmental Control 16(3): 231-256.
Shuval, H.I. 1991. Health guidelines and standards for wastewater reuse in agriculture: historical perspectives. Water Science and Technology 23(10/12): 2037-2080.
Shuval, H.I. 1993. Investigation of typhoid fever and cholera transmission by raw wastewater irrigation in Santiago, Chile. Water Science and Technology 27(3/4): 167-174.
Shuval, H.I., Adin, A., Fattal, B., Rawitz, E. and Yekutiel, P. 1986a. Wastewater irrigation in developing countries: health effects and technical solutions. Technical Paper Number 51. World Bank, Washington DC. 324 p.
Shuval, H.I., Yekutiel, P. and Fattal, B. 1986b. An epidemiological model of the potential health risk associated with various pathogens in wastewater irrigation. Water Science and Technology 18(10): 191-198.
Strauss, M. 1985. Health Aspects of Nightsoil and Sludge Use in Agriculture and Aquaculture. Part II: Pathogen Survival (Report No. 04/85). International Reference Center for Waste Disposal. Dubendorf.
Strauss, M. 1991. Human waste use: health protection practices and scheme monitoring. Water Science and Technology 24(9): 67-79.
US EPA. 1973. Water Quality Criteria. National Academy of Sciences Report to the United States Environmental Protection Agency. Washington DC. pp. 350-366.
WHO. 1973. Reuse of effluents: methods of wastewater treatment and health safeguards: Report of a WHO Meeting of Experts. WHO Technical Report Series No. 517. World Health Organization, Geneva.
WHO. 1989. Health guidelines for the use of wastewater in agriculture and aquaculture: Report of a WHO Scientific Group. WHO Technical Report Series 778. World Health Organization, Geneva. 74 p.
Witt, V.M. and Reiff, F.M. 1991. Environmental health conditions and cholera vulnerability in Latin America and the Caribbean. Journal of Public Health Policy 12(4): 450-463.
FAO TECHNICAL PAPERS
WATER REPORTS
1. Prevention of water pollution by agriculture and related activities, 1993 (E S)
2. Irrigation water delivery models, 1994 (E)
3. Water harvesting for improved agricultural production, 1994 (E)
4. Use of remote sensing techniques in irrigation and drainage, 1995 (E)
5. Irrigation management transfer, 1995 (E)
6. Methodology for water policy review and reform, 1995 (E)
7. Irrigation in Africa in figures/L'irrigation en Afrique en chiffres, 1995 (E/F)
8. Irrigation scheduling: from theory to practice, 1996 (E)
9. Irrigation in the Near East Region in figures, 1997 (E)
10. Quality control of wastewater for irrigated crop production, 1997 (E)
Availability: June 1997
Ar - Arabic
C - Chinese
E - English
F - French
P - Portuguese
S - Spanish
Multil - Multilingual
* Out of print
** In preparation
The FAO Technical Papers are available through the authorized FAO Sales Agents or directly from Sales and Marketing Group, FAO, Viale delle Terme di Caracalla. 00100 Rome, Italy.
This document discusses the use of wastewater for irrigated crop production. It reviews wastewater standards and proposes an interim approach to be applied in areas using wastewater, which promotes safe production areas for crops such as vegetables. The approach is to assess the quality of water actually being used for irrigation against a known standard. It is proposed that the World Health Organization (WHO) guidelines for wastewater treatment plant design be used as irrigation water standards in making this assessment. In view of the fact that the present lever of water contamination in many countries already seriously exceeds the limits set in the guidelines, achieving the prescribed standards for vegetable production would be a major accomplishment towards improving health conditions in these countries. The document makes reference to procedures developed and studied in 1992 in an FAO project in Chile. Comments and suggestions for improvement of this approach for
practical application in the field are invited and encouraged.
